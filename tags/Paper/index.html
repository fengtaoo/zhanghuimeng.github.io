<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  <title>Tag: Paper | 张慕晖的博客</title>
  
  

  
  <link rel="alternate" href="/atom.xml" title="张慕晖的博客">
  

  <meta name="HandheldFriendly" content="True" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <!-- meta -->
  

  <!-- link -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-waves@0.7.6/dist/waves.min.css">
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.6.3/css/all.min.css">
  
  
  <link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css'>
  

  
  <link rel="shortcut icon" type='image/x-icon' href="/files/favicon.ico">
  

  
  <link rel="stylesheet" href="/style.css">
  

  <script>
    function setLoadingBarProgress(num) {
      document.getElementById('loading-bar').style.width=num+"%";
    }
  </script>

  
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-119345306-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-119345306-1');
    </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  
  
</head>

<body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="loading-bar-wrapper">
  <div id="loading-bar" class="pure"></div>
</div>

    <script>setLoadingBarProgress(20)</script>
    <header class="l_header pure">
	<div class='wrapper'>
		<div class="nav-main container container--flex">
      <a class="logo flat-box" href='/' >
        
          张慕晖的博客
        
      </a>
			<div class='menu'>
				<ul class='h-list'>
          
				</ul>
			</div>

			
				<div class="m_search">
					<form name="searchform" class="form u-search-form">
						<input type="text" class="input u-search-input" placeholder="搜索" />
						<span class="icon"><i class="fas fa-search fa-fw"></i></span>
					</form>
				</div>
			
			<ul class='switcher h-list'>
				
					<li class='s-search'><a class="fas fa-search fa-fw" href='javascript:void(0)'></a></li>
				
				<li class='s-menu'><a class="fas fa-bars fa-fw" href='javascript:void(0)'></a></li>
			</ul>
		</div>

		<div class='nav-sub container container--flex'>
			<a class="logo flat-box"></a>
			<ul class='switcher h-list'>
				<li class='s-comment'><a class="flat-btn fas fa-comments fa-fw" href='javascript:void(0)'></a></li>
				<li class='s-toc'><a class="flat-btn fas fa-list fa-fw" href='javascript:void(0)'></a></li>
			</ul>
		</div>
	</div>
</header>
	<aside class="menu-phone">
    <header>
		<nav class="menu">
      <ul>
          
      </ul>
		</nav>
    </header>
	</aside>

    <script>setLoadingBarProgress(40);</script>
    <div class="l_body">
    <div class='container clearfix'>
        <div class='l_main'>
            
    <script>
        window.subData= { title:'标签 : Paper'}
    </script>




  <section class="post-list">
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      



      

      

      
        
          
            <div class='post-wrapper'>
              <article class="post reveal ">
  
<section class='meta'>
  
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/post/deepquest-a-framework-for-neural-based-quality-estimation/">
              
                  论文：deepQuest: A Framework for Neural-based Quality Estimation
              
          </a>
      </h2>
    

    <div class='new-meta-box'>
      
        <div class='new-meta-item author'>
          <a href="https://zhanghuimeng.github.io">
            <i class="fas fa-user" aria-hidden="true"></i>
            张慕晖
          </a>
        </div>
      
      
        <div class="new-meta-item date">
          <a class='notlink'>
            <i class="fas fa-calendar-alt" aria-hidden="true"></i>
            2018-12-03
          </a>
        </div>
      
      
        
      
      
      
    </div>
    <hr>
  </div>
</section>

  <section class="article typo">
    <div class="article-entry" itemprop="articleBody">
      <p>论文地址：<a href="http://aclweb.org/anthology/C18-1266" target="_blank" rel="noopener">http://aclweb.org/anthology/C18-1266</a></p>
<p>这篇文章的主要内容是实现了一个新的叫<a href="https://sheffieldnlp.github.io/deepQuest/introduction.html" target="_blank" rel="noopener">deepQuest</a>的QE系统（已开源），里面包含了之前sentence-level QE的SOTA（现在已经不是了），POSTECH和他们这次新发明的Bi-RNN两个sentence-level的系统，以及在这两个sentence-level基础上的document-level QE系统，发布了一个新的<a href="https://github.com/fredblain/docQE" target="_blank" rel="noopener">Resources for document-level Quality Estimation</a>数据集（也已开源），并在WMT 17和上述数据集上报告了结果。结果表明，Bi-RNN在sentence-level上效果不如POSTECH（但是训练快），作为document-level的基础系统效果更好。</p>
<h2>论文内容</h2>
<p>sentence-level QE通常预测的是机器翻译输出结果的post-editing effort，但document-level QE通常是在全自动的MT应用场景下预测文档质量分数（而非PE）。目前所有的神经网络QE方法都有复杂的结构，需要预训练和手动提取feature，而且没有document-level的方法。（吐槽：事实上我觉得预训练和复杂的网络结构都是必要的，甚至是很必要的……）所以我们提出了一个新的sentence-level方法（Bi-RNN），它不需要预训练，结构很简单，训练起来很快。我们还提出了一个新的神经网络的document-level的方法，它可以将任何细粒度的QE方法的结果进行综合，得到更粗粒度的QE结果，比如将sentence-level的输出综合处理得到document-level的结果。经过在sent-level和doc-level的一些欧洲语言的SMT和NMT上输出的测试，我们发现，对高质量NMT输出进行QE的主要挑战是在已经很流利的文本中找出错误。</p>
<h3>Sentence-level</h3>
<p>作者首先实现了WMT 17的冠军系统POSTECH。这个系统分成两个部分：</p>
<ul>
<li>predictor：encoder-decoder RNN，基于context对词进行预测，生成表示向量</li>
<li>estimator：Bi-RNN，根据predictor生成的表示向量进行打分</li>
</ul>
<p>其中predictor部分需要大量预训练。（这就和<a href="/post/bilingual-expert-can-find-translation-errors">QEBrain</a>的思路基本上是一样的，只不过具体用的架构不太一样。我可能还是需要去读一下具体是怎么做的。）</p>
<p>然后实现了新的一种架构，Bi-RNN。这个方法的思路非常简单：</p>
<ul>
<li>对输入和输出分别进行word embedding</li>
<li>分别用一个bi-RNN对输入和输出的embedding进行学习，得到一系列隐状态$h_j$（两个RNN分开训练，但是输出结果连在一起，一起进行attention）</li>
<li>对所有的隐状态进行attention，得到加权后的和</li>
<li>进行sigmoid，将输出作为分数</li>
</ul>
<p>attention的公式为：</p>

$$
\alpha_j = \frac{\exp{(W_a h_j^T)}}{\sum_{k=1}^{J} \exp{(W_a h_j^T)}} \\
v = \sum_{j=1}^J \alpha_j h_j
$$

<p><img src="sent.jpg" alt="Bi-RNN结构图"></p>
<h3>Document-level</h3>
<p>之前我们可以看到，Bi-RNN输出了一个$v$向量，实际上可以把它看做是一个对整个句子的表示。POSTECH中Predictor也会输出一个类似的向量。我们可以把这些向量表示再做一次Bi-RNN，对结果进行attention（或者直接用最后一个隐状态），得到整个文档对应的向量，再对这个向量进行sigmoid，得到文档对应的分数。</p>
<p><img src="doc.jpg" alt="document-level结构图"></p>
<h2>测试结果</h2>
<h3>Sentence-level</h3>
<p>测试使用的是WMT 2017 QE的官方数据的一个超集，包括NMT和SMT的翻译结果，其中包括：</p>
<ul>
<li>En-De：28000句（IT领域）</li>
<li>En-Lv：18768句（生命科学领域）</li>
</ul>
<p>数据使用TERCOM进行标注，分数采用HTER。POSTECH的predictor使用Europarl和WMT 2017 News translation Task的语料进行预训练。</p>
<p>一些实现细节（当然，代码里有更多的细节）：</p>
<ul>
<li>使用Keras进行实现（而且似乎支持Theano和TensorFlow两种backend）</li>
<li>使用GRU作为RNN单元</li>
<li>word embedding维度为300</li>
<li>词表大小为30K</li>
<li>encoder隐藏层单元大小为50</li>
<li>用Adadelta optimizer最小化MSE</li>
</ul>
<p><img src="table-1.png" alt="sent-level结果"></p>
<p>可以得出以下结论：</p>
<ul>
<li>En-De数据集中NMT的翻译质量高于SMT，而En-Lv数据集中NMT的翻译质量不如SMT，这影响了各个方法的表现</li>
<li>POSTECH方法在SMT数据上的表现比在NMT数据上高40%，这可能受到了数据质量或者句子长度的影响</li>
<li>无预训练的POSTECH方法表现不如Bi-RNN，这可能是因为Bi-RNN能够把握NMT数据的流利度</li>
<li>（但是，还是有预训练的POSTECH方法效果最好）</li>
</ul>
<h3>Document-level</h3>
<p>预测的分数采用的是几种BLEU值：</p>
<ul>
<li>document-level BLEU（使用NLTK进行打分）</li>
<li>wBLEU：文档中句子BLEU值的加权平均，权重是句子长度</li>
<li>tBLEU：也是句子的BLEU值的加权平均，但是权重是TFIDF；对每个文档，都学习一个新的TFIDF模型，并据此计算TFIDF分数</li>
</ul>

$$
\text{wBLEU}_d = \frac{\sum_{i=1}^D \text{len}(R_i)\text{BLEU}_i}{\sum_{i=1}^D \text{len}(R_i)}
$$

$$
\text{tBLEU} = \sum_{i=1}^D \text{TFIDF}_i \text{BLEU}_i
$$


<p>在document-level的测试中，使用的数据是WMT News Task中这几年提交的机器翻译结果，作者对数据进行了一定筛选，并把相应的数据集开源了（<a href="https://github.com/fredblain/docQE" target="_blank" rel="noopener">docQE</a>）。</p>
<p><img src="table-2.png" alt="数据集的情况"></p>
<p>document-level系统使用的是POSTECH/Bi-RNN系统输出的句子表示，并测试了使用attention/只使用最后一个状态的结果。</p>
<p><img src="table-4.png" alt="测试结果"></p>
<p>通过上述结果可以发现：</p>
<ul>
<li>tBLEU标记下QE系统表现最好，BLEU标记下QE系统表现最差。这可能是因为tBLEU的计算方式和这种从word representation到sentence到document的结构是最类似的。</li>
<li>以Bi-RNN作为输入representation的效果整体优于POSTECH，而且训练得还快。</li>
<li>attention的贡献不是统计显著的，这可能是因为相关距离会变化，很难找到最优的权重。</li>
<li>单独筛选训练数据不会提高表现。</li>
<li>POSTECH非常需要预训练。</li>
<li>De-En和En-Ru的预测难度最大，这可能是因为语言之间词序差异很大，相关的MT输出质量通常也比较低。</li>
</ul>
<h2>运行结果</h2>
<p>TBD</p>

      
    </div>
    
      <div class="full-width auto-padding tags">
        
          <a href="/tags/Natural-Language-Processing/"><i class="fas fa-hashtag fa-fw"></i>Natural Language Processing</a>
        
          <a href="/tags/Reading-Report/"><i class="fas fa-hashtag fa-fw"></i>Reading Report</a>
        
          <a href="/tags/Machine-Translation/"><i class="fas fa-hashtag fa-fw"></i>Machine Translation</a>
        
          <a href="/tags/Paper/"><i class="fas fa-hashtag fa-fw"></i>Paper</a>
        
          <a href="/tags/Quality-Estimation/"><i class="fas fa-hashtag fa-fw"></i>Quality Estimation</a>
        
      </div>
    
  </section>
</article>

            </div>
          
        
          
            <div class='post-wrapper'>
              <article class="post reveal ">
  
<section class='meta'>
  
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/post/contextual-encoding-for-translation-quality-estimation/">
              
                  论文：Contextual Encoding for Translation Quality Estimation
              
          </a>
      </h2>
    

    <div class='new-meta-box'>
      
        <div class='new-meta-item author'>
          <a href="https://zhanghuimeng.github.io">
            <i class="fas fa-user" aria-hidden="true"></i>
            张慕晖
          </a>
        </div>
      
      
        <div class="new-meta-item date">
          <a class='notlink'>
            <i class="fas fa-calendar-alt" aria-hidden="true"></i>
            2018-11-10
          </a>
        </div>
      
      
        
      
      
      
    </div>
    <hr>
  </div>
</section>

  <section class="article typo">
    <div class="article-entry" itemprop="articleBody">
      <p>论文地址：<a href="https://arxiv.org/pdf/1809.00129.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1809.00129.pdf</a></p>
<p>这篇文章也是WMT18 QE Task的提交系统之一，在word-level task上取得了较好的效果（虽然我觉得一部分原因是QEBrain和UNQE没有参加一部分task）。它的主要思路是在之前的一篇文章<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>的基础上做了改进，用卷积层起到类似于attention的作用。这个思路的效果看起来不如一般的attention。（而且文章写得一点也不清楚，代码库也找不到，我还是不知道模型的具体结构是什么样的。）</p>
<h2>模型结构</h2>
<p><img src="architecture.png" alt="模型结构"></p>
<p>（图里的模型结构画得一点也不确切……）</p>
<p>这篇文章的模型是在<sup class="footnote-ref"><a href="#fn1" id="fnref1:1">[1]</a></sup>的基础上改进出来的，中间多加了一层卷积。</p>
<p>模型的输入是三元组$\langle s, t, \mathcal{A} \rangle$，其中$s = s_1, ..., s_M$是源句，$t = t_1, ..., t_N$是译句，$\mathcal{A} \subseteq {(m, n) | 1 \leq m \leq M, 1 \leq n \leq N}$是alignment。</p>
<p>模型分成三个主要部分：</p>
<ul>
<li>词和POS的embedding层</li>
<li>卷积层</li>
<li>RNN和FF层</li>
</ul>
<p>embedding层的vector是这样构成的（$\Vert$表示row-wise concatenation）：</p>
<ul>
<li>记embedding的维度为$d$，令源词和译词采用同样的embedding参数，记源词的embedding为$e_{s_i}$，译词的embedding为$e_{t_j}$</li>
<li>将每个译词自己的embedding和与它对齐的源词的embedding的平均值连接在一起，得到$\mathbf{x}'_j = ave(e_{s_{\mathcal{A}(:, t_j)}}) \Vert e_{t_j}$，这是一个长度为$2d$的向量</li>
<li>将$\mathbf{x}'_j$和$\mathbf{x}'_{j-1}$和$\mathbf{x}'_{j+1}$连接在一起，得到$\mathbf{x}_j = \mathbf{x}'_{j-1} \Vert \mathbf{x}'_j \Vert \mathbf{x}'_{j+1}$，这是一个长度为$6d$的向量</li>
</ul>
<p>然后将这些vector连接在一起进行卷积（$\oplus$表示column-wise concatenation）：</p>
<ul>
<li>将$\mathbf{x}_j$连接在一起，构成一个矩阵：$\mathbf{x}_{1:N} = \mathbf{x}_1 \oplus \mathbf{x}_2 ... \oplus \mathbf{x}_N$</li>
<li>然后对上述矩阵进行一维卷积：$c_i = f(\mathbf{w} \cdot \mathbf{x}_{i:i+h-1} + b)$，得到feature$\mathbf{c} = {c_1, c_2, ..., c_N}$（进行了padding）</li>
<li>在不同的窗口大小下（$\mathcal{H} = {1, 3, 5, 7}$）各学习$n_f = 64$个feature，将这些feature连接起来，得到$C \in \mathbb{R}^{N \times |\mathcal{H}| \cdot n_f}$的卷积层输出，相当于每个词由长度为$\mathcal{H}| \cdot n_f = 256$的向量表示（这句是我猜的）</li>
<li>最后再把上述向量和译词的POS tag embedding和与译词对齐的源词的POS tag embedding连接起来（文中没有说POS tag是怎么来的，但<sup class="footnote-ref"><a href="#fn1" id="fnref1:2">[1]</a></sup>中是用TurboTagger标记的；我猜可能也要进行平均）</li>
</ul>
<p>然后对每个词对应的向量表示进行处理（似乎使用了stacked RNN的方法，我还没太搞懂）：</p>
<ul>
<li>两层FF（ReLU），隐藏层大小为400</li>
<li>一层Bi-GRU，隐藏层大小为200，前向表示和后向表示连接后进行layer normalization</li>
<li>两层FF（ReLU），隐藏层大小为200</li>
<li>一层Bi-GRU，隐藏层大小为100，同样进行layer normalization</li>
<li>一层FF（ReLU），隐藏层大小为100</li>
<li>一层FF（ReLU），隐藏层大小为50</li>
</ul>
<p>（我猜测每个词对应的网络参数是相同的？）</p>
<p>然后把最后一层输出的FF feature和Marmot输出的31个baseline feature连接起来，通过softmax来预测OK/BAD label。</p>
<h2>实验结果</h2>
<p><img src="result.png" alt="实验结果"></p>
<p>这个模型取得了比较好的结果。</p>
<p>作者还进行了一些sensitivity analysis（调整dropout rate）和ablation analysis（删除模型中的一些部分，观察效果），具体内容就不写了。</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p><a href="http://www.aclweb.org/anthology/Q17-1015" target="_blank" rel="noopener">Pushing the Limits of Translation Quality Estimation</a> <a href="#fnref1" class="footnote-backref">↩</a> <a href="#fnref1:1" class="footnote-backref">↩</a> <a href="#fnref1:2" class="footnote-backref">↩</a></p>
</li>
</ol>
</section>

      
    </div>
    
      <div class="full-width auto-padding tags">
        
          <a href="/tags/Natural-Language-Processing/"><i class="fas fa-hashtag fa-fw"></i>Natural Language Processing</a>
        
          <a href="/tags/Reading-Report/"><i class="fas fa-hashtag fa-fw"></i>Reading Report</a>
        
          <a href="/tags/Machine-Translation/"><i class="fas fa-hashtag fa-fw"></i>Machine Translation</a>
        
          <a href="/tags/Paper/"><i class="fas fa-hashtag fa-fw"></i>Paper</a>
        
          <a href="/tags/Quality-Estimation/"><i class="fas fa-hashtag fa-fw"></i>Quality Estimation</a>
        
      </div>
    
  </section>
</article>

            </div>
          
        
          
            <div class='post-wrapper'>
              <article class="post reveal ">
  
<section class='meta'>
  
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/post/a-unified-neural-network-for-quality-estimation-of-machine-translation/">
              
                  论文：A Unified Neural Network for Quality Estimation of Machine Translation
              
          </a>
      </h2>
    

    <div class='new-meta-box'>
      
        <div class='new-meta-item author'>
          <a href="https://zhanghuimeng.github.io">
            <i class="fas fa-user" aria-hidden="true"></i>
            张慕晖
          </a>
        </div>
      
      
        <div class="new-meta-item date">
          <a class='notlink'>
            <i class="fas fa-calendar-alt" aria-hidden="true"></i>
            2018-11-07
          </a>
        </div>
      
      
        
      
      
      
    </div>
    <hr>
  </div>
</section>

  <section class="article typo">
    <div class="article-entry" itemprop="articleBody">
      <p>论文地址：<a href="https://www.jstage.jst.go.jp/article/transinf/E101.D/9/E101.D_2018EDL8019/_article/-char/en" target="_blank" rel="noopener">https://www.jstage.jst.go.jp/article/transinf/E101.D/9/E101.D_2018EDL8019/_article/-char/en</a></p>
<p>这篇论文描述了一种新的用于Quality Estimation的神经网络结构，在WMT18 QE Task中取得了较好的成绩（仅次于QEBrain，同样大大超过了SOTA）。</p>
<h2>相关工作</h2>
<p>传统的QE方法是把它看成是一个有监督回归/分类模型。典型的做法如QuEst：先从输入中提取feature，再根据feature用SVR进行打分。这种做法存在的问题之一是，提取feature的过程与源语言本身密切相关，因此限制了它在不同语言中的应用。</p>
<p>之后研究者们开始在QE中应用deep learning，可以分成两大类：</p>
<ul>
<li>neural-aware QE：将neural feature（如word embedding、translation condition probability、cross entropy等）集成到QE系统中，可以有效提高系统表现。</li>
<li>pure neural QE：直接建立一个用于QE任务的神经网络；目前的SOTA是用两个分开的网络（RNNsearch predictor + RNN estimator）分别进行训练，然后输出结果。这种方法的表现比neural-aware QE更好。</li>
</ul>
<p>本文中的做法是将RNNsearch和RNN组成一个整体的网络，共同进行训练。</p>
<p>（之后的实验结果将说明，pure neural QE &gt; neural-aware QE &gt; traditional QE，而在pure neural QE中，unified network又好于separated network。）</p>
<h2>模型结构</h2>
<p><img src="fig-1-mode-architecture.png" alt="模型结构图"></p>
<p>（这篇文章把模型结构讲得比较细。）</p>
<p>模型分成两个主要模块：</p>
<ul>
<li>RNNsearch：从句对中提取quality vector</li>
<li>RNN：用quality vector对翻译质量进行预测，可以看成是有监督回归任务</li>
</ul>
<p>这两个模块共同进行训练。其中RNNsearch中间生成的context vector是$c_1, ..., c_n$，decoder RNN的隐状态是$s_0, s_1, ..., s_n$，$t_1, ..., t_j$是中间表示，可以通过下式计算：</p>

$$t_j = \tanh{(U_o s_{j-1} + V_o E y_{j-1} + C_o c_j)}$$

<p>其中$U_o, V_o, C_o$是模型参数，$E$是目标语言的embedding矩阵。</p>
<p>给定输入$(x_1, ..., x_m)$，decoder将生成翻译输出$(y_1, ..., y_n)$，其中生成每个词的条件概率为：</p>

$$p(y_j | \{y_1, ..., y_{j-1}\}, x) = g(y_{j-1}, s_{j-1}, c) = \frac{\exp{(y^T_j W_o t_j)}}{\sum_{k=1}^{K_y} \exp{(y_k^T W_o t_j)}}$$

<p>其中$W_o$是权重矩阵。（显然上式只是对$y^T_j W_o t_j$做了一个softmax。）为了通过上述条件概率对翻译质量进行描述，可以这样计算quality vector：</p>
<p><img src="fig-2-calc-quality-vector.png" alt="计算quality vector的图示"></p>

$$q_{y_j} = [(y^T_j W_o) \odot t^T_j]^T$$

<p>（所以这里就直接用了翻译条件概率……）</p>
<p>最后将这些quality vector依次输入到RNN（作者使用的是GRU单元）中，将最后一个输出作为QE得分：</p>

$$v_j = f(v_{j-1}, q_{y_j})$$
$$QE_{score} = W_{QE} \times v_n$$

<p>其中$W_{QE}$是权重矩阵。最终分数没有用logistic sigmoid函数进行平均，而是直接进行了clip。</p>
<h2>模型训练</h2>
<p>由于QE任务的训练集太小了，因此RNNsearch和QE RNN先分别用平行语料（WMT17翻译任务的训练语料）和QE训练语料进行了预训练，然后才共同用QE训练语料进行训练。训练目标是最小化MAE：</p>
<p>$$J(\theta) = \frac{1}{N} \sum{n=1}^{N} |QE_{score}(x^{(n)}, y^{(n)}, \theta) - HTER^{(n)}|$$</p>
<p>由于模型的各部分是一起训练的，因此输出的quality vector是可以进行训练的（而不是像estimator-predictor方法中，RNNsearch的输出是固定的翻译结果），能够提取出更准确的feature。</p>
<h2>实验结果</h2>
<p>表中列出的系统包括：</p>
<ul>
<li>QuEst：传统QE方法</li>
<li>SHEF/QUEST-EMB：neural-aware QE方法</li>
<li>JXNU/Emb+RNNLM+QuEst+SNM：neural-aware QE方法</li>
<li>Predictor-Estimator：pure neural QE方法</li>
<li>UNQE：本文的方法</li>
</ul>
<p><img src="result.png" alt="实验结果"></p>
<p>分析结果可以得到以下结论：</p>
<ul>
<li>pure neural QE方法好于neural-aware QE方法好于传统QE方法</li>
<li>UNQE方法好于Predictor-Estimator方法</li>
<li>ensemble是有效的</li>
<li>对预测分数进行logistic sigmoid不如直接进行clip</li>
</ul>
<h2>一些想法</h2>
<p>这篇文章的做法是直接利用RNNsearch的翻译结果（或者说生成翻译的条件概率）进行Quality Estimation，那么它和“直接用一个最强的翻译系统进行翻译然后比较翻译结果和实际翻译输出”有什么差异呢？文中也讲到，如果真的直接用一个RNNsearch系统的翻译输出作为feature然后去预测，效果是不如像这个系统这样，将RNNsearch + QE RNN共同进行训练的。我猜测原因可能包括：</p>
<ol>
<li>RNNsearch还是不够好，应该尝试直接用Transformer的翻译输出作为feature进行预测，然后再将结果和QEBrain进行比较</li>
<li>MT系统内部训练时计算loss的方式（应该是cross entropy吧）和HTER打分是有差异的，因此需要额外的训练，使得它不止可以输出正确的翻译，还可以对翻译的实际得分有更好的估计</li>
</ol>
<p>至于模型本身的结构，作者似乎没有像QEBrain那样考虑不同的estimator结构对结果的影响，如果尝试不同的结构（如LSTM、Bi-LSTM等）是否效果会更好？</p>
<p>以及一个问题：HTER是对翻译质量的最好的度量方式吗？如果直接换成人类打分（像Task3和Task4那样）会怎样？</p>

      
    </div>
    
      <div class="full-width auto-padding tags">
        
          <a href="/tags/Natural-Language-Processing/"><i class="fas fa-hashtag fa-fw"></i>Natural Language Processing</a>
        
          <a href="/tags/Reading-Report/"><i class="fas fa-hashtag fa-fw"></i>Reading Report</a>
        
          <a href="/tags/Machine-Translation/"><i class="fas fa-hashtag fa-fw"></i>Machine Translation</a>
        
          <a href="/tags/Paper/"><i class="fas fa-hashtag fa-fw"></i>Paper</a>
        
          <a href="/tags/Quality-Estimation/"><i class="fas fa-hashtag fa-fw"></i>Quality Estimation</a>
        
      </div>
    
  </section>
</article>

            </div>
          
        
          
            <div class='post-wrapper'>
              <article class="post reveal ">
  
<section class='meta'>
  
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/post/bilingual-expert-can-find-translation-errors/">
              
                  论文：“Bilingual Expert” Can Find Translation Errors
              
          </a>
      </h2>
    

    <div class='new-meta-box'>
      
        <div class='new-meta-item author'>
          <a href="https://zhanghuimeng.github.io">
            <i class="fas fa-user" aria-hidden="true"></i>
            张慕晖
          </a>
        </div>
      
      
        <div class="new-meta-item date">
          <a class='notlink'>
            <i class="fas fa-calendar-alt" aria-hidden="true"></i>
            2018-11-03
          </a>
        </div>
      
      
        
      
      
      
    </div>
    <hr>
  </div>
</section>

  <section class="article typo">
    <div class="article-entry" itemprop="articleBody">
      <p>论文地址：<a href="https://arxiv.org/pdf/1807.09433.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1807.09433.pdf</a></p>
<p>这篇文章描述了2018 WMT Quality Estimation中效果最好的系统。作者认为他们的工作有以下几项主要贡献：</p>
<ol>
<li>提出了一个新的经过预训练的基于Bi-Transformer的prior-knowledge模型，且可以用于APE（auto post-editing）</li>
<li>提出了3种mis-match feature</li>
<li>用Bi-LSTM通过输入特征进行quality estimation</li>
<li>提出了一种在计算流图中使用BPE的方法</li>
</ol>
<h2>Quality Estimation的传统方法和形式定义</h2>
<p>一种传统方法是把预测sentence-level得分看做是一个constraint regression问题，把预测word-level标签看做是sequence labeling问题；然后先提取特征，再对翻译质量进行预测。</p>
<p>从统计学的角度，可以把翻译系统形式化地看成是$p(\mathbf{t} | \mathbf{s}) = p(\mathbf{t} | \mathbf{z}) p(\mathbf{z} | \mathbf{s})$，其中$\mathbf{s}$表示源句的token sequence，$\mathbf{t}$表示译句，$\mathbf{z}$是表示encode过的源句的隐变量。因此，可以把$p(\mathbf{z} | \mathbf{s})$看成是encoder，$p(\mathbf{t} | \mathbf{z})$看成是decoder。</p>
<p>在QE任务中，MT系统是未知的，输入数据是$(\mathbf{s}, \mathbf{m}, \mathbf{t})$，其中$\mathbf{m}$是未知系统的输出，$\mathbf{t}$是对$\mathbf{m}$ post-edit之后的结果。一般来说，至少可以在两个层面对$\mathbf{m}$进行评价：</p>
<ul>
<li>word-level：根据$\mathbf{m}$和$\mathbf{t}$对$\mathbf{m}$中的token生成的OK/BAD标签</li>
<li>sentence-level：根据$\mathbf{m}$和$\mathbf{t}$之间的差异计算HTER分数</li>
</ul>
<p>因此可以假定训练数据实际上是$(\mathbf{s}, \mathbf{m}, \mathbf{t}, h, \mathbf{y})$，其中$h$是HTER，$\mathbf{y}$是OK/BAD标签。QE任务即训练回归模型$p(h | \mathbf{s}, \mathbf{m})$和sequence labeling模型$p(\mathbf{y} | \mathbf{s}, \mathbf{m})$。</p>
<p>（这项工作的架构类似于predictor-estimator的架构，但是两边是共同训练的，因此效果得到提升。）</p>
<h2>Conditional Language/Feature Extration Model: Bilingual Expert Model</h2>
<p>作者首先形式化地描述了如何训练这个模型（实际上我没太看懂）。</p>
<p>通过贝叶斯公式，可以写出隐变量$\mathbf{z}$的后验分布公式：</p>
<p>$$p(\mathbf{z} | \mathbf{t}, \mathbf{s}) = \frac{p(\mathbf{t} | \mathbf{z}) p(\mathbf{z} | \mathbf{s})}{p(\mathbf{t} | \mathbf{s})}$$</p>
<p>由于$p(\mathbf{t} | \mathbf{s})$无法直接计算，因此用分布$q(\mathbf{z} | \mathbf{t}, \mathbf{s})$通过最小化KL散度来逼近实际的后验：</p>
<p>$$\min{D_{KL} (q(\mathbf{z} | \mathbf{t}, \mathbf{s}) || p(\mathbf{z} | \mathbf{t}, \mathbf{s}))}$$</p>
<p>可以把上述目标函数换成下面这个（我猜这和KL散度的性质有关，但我不会）：</p>

$$\max{\mathbb{E}_{q(\mathbf{z} | \mathbf{t}, \mathbf{s})}[p(\mathbf{t} | \mathbf{z})] - D_{KL} (q(\mathbf{z} | \mathbf{t}, \mathbf{s}) || p(\mathbf{z} | \mathbf{s}))}$$

<p>这个新目标函数的优点是不需要直接估计机器翻译模型$p(\mathbb{t} | \mathbb{s})$。（至少在这里仍然把MT和QE区分开了，但在实际模型架构中几乎是一样的。）而且可以直接计算$p(\mathbf{z} | \mathbf{s})$；左边的期望似然实际上是一个VAE（variational autoencoder），可以进行估计：</p>

$$\max{\mathbb{E}_{q(\mathbf{z} | \mathbf{t}, \mathbf{s})}[p(\mathbf{t} | \mathbf{z})] \approx p(\mathbf{t} | \mathbf{\tilde{z}})}, \quad \tilde{z} \sim q(\mathbf{z} | \mathbf{t}, \mathbf{s})$$

<p>下面只需通过Transformer构造出$p(\mathbf{t} | \mathbf{z})$和$q(\mathbf{z} | \mathbf{t}, \mathbf{s})$这两个概率。</p>
<hr>
<p>Transformer部分模型架构如下图右侧部分：</p>
<p><img src="qebrain-mode.png" alt="模型整体结构"></p>
<p>其中有三个主要模块：</p>
<ul>
<li>self-attention encoder：encode源句</li>
<li>前向+后向self-attention encoder：encode译句</li>
<li>reconstructor：重新生成译句</li>
</ul>
<p>（这架构和Transformer几乎没有区别……除了译句encoder换成了双向的）</p>
<p>将$p(\mathbf{t} | \mathbf{z})$和$q(\mathbf{z} | \mathbf{t}, \mathbf{s})$进行如下分解：</p>

$$p(\mathbf{t} | \mathbf{z}) = \prod_{k} p(t_k | \overrightarrow{\mathbf{z}_k}, \overleftarrow{\mathbf{z}_k})$$

$$q(\mathbf{z} | \mathbf{t}, \mathbf{s} = \prod_{k} q(\overrightarrow{\mathbf{z}_k} | \mathbf{s}, \mathbf{t}_{< k}, \overleftarrow{\mathbf{z}_k} | \mathbf{s}, \mathbf{t}_{> k})$$

<h2>三种特征</h2>
<p>在训练完expert model之后，可以从中提取句对$(\mathbf{s}, \mathbf{m})$中的3种特征：</p>
<ul>
<li>隐变量$\mathbf{z}_k$：它应当包含了源句和译句的所有信息，以及正确翻译第k个token所需的语义信息</li>
<li>token embedding：对于第k个token，使用它前后token的embedding，即$(\mathbf{e}_{t_{k-1}}, \mathbf{e}_{t_{k+1}})$</li>
<li>分类分布：令$p(t_k | \cdot)$为类别数量与词表大小相等的分类分布（categorical distribution），则$p(t_k | \cdot) \sim \text{Categorical}(softmax(\mathbf{I}_k))$</li>
</ul>
<p>于是可以构造4维的mis-matching feature：</p>

$$\mathbf{f}_k^{mm} = (\mathbf{I}_{k, m_k}, \mathbf{I}_{k, i_{max}}, \mathbf{I}_{k, m_k} - \mathbf{I}_{k, i_{max}}, \mathbb{I}_{m_k \neq i_{max}})$$

<p>其中$m_k$是翻译输出中的第k个token，$i_max = \arg\max_i{\mathbf{I}_k}$为expert model预期的输出token，$\mathbb{I}$是概率分布。</p>
<p>（这预期输出token基本上就相当于是自己翻译了一遍吧……）</p>
<h2>Bi-LSTM Quality Estimation</h2>
<p>实验中发现encoder self-attention、Bi-Transformer和Bi-LSTM+CRF的效果都不如普通的Bi-LSTM，可能是因为训练数据不够多。因此直接使用了Bi-LSTM。</p>
<p>Bi-LSTM输入expert model生成的feature：</p>

$$\overrightarrow{\mathbf{h}_{1:T}}, \overleftarrow{\mathbf{h}_{1:T}} = \text{Bi-LSTM}(\{\mathbf{f}_k\}_{k=1}^T)$$

<p>然后通过回归方法预测HTER（因为HTER是整个句子的评分，因此只考虑Bi-LSTM的最后两个状态）：</p>

$$\arg\min{\lVert h - \text{sigmoid}(\mathbf{w}^T [\overrightarrow{\mathbf{h}_{T}}, \overleftarrow{\mathbf{h}_{T}}]) \rVert^2_2}$$

<p>通过sequence labeling方法预测词的标签（其中XENT是cross entropy）：</p>

$$\arg\min{\sum_{k=1}^T \text{XENT}(y_k, \mathbf{W}[\overrightarrow{\mathbf{h}_{k}}, \overleftarrow{\mathbf{h}_{k}}])}$$

<h2>计算流图中的BPE</h2>
<p>对于sentence-level QE，由于HTER是全局参数，显然使用BPE和不使用BPE没有太大的差别。但是对于word-level QE，BPE下序列的长度$L_b \neq L_{\omega}$（原序列长度）。因此作者提出了一种对一个词的所有subword unit的feature进行平均的方法：将BPE信息存储在一个$L_{\omega} \times L_b$的稀疏矩阵$S$中，其中仅当第j个subword unit属于第i个词时，$s_{ij} \neq 0$。此时就可以通过矩阵乘法计算平均后的feature，这使得计算流图是可微的。</p>
<p>通过这一方法使用BPE后，效果有一定的提升。</p>
<p><img src="bpe.png" alt="左侧为矩阵，右侧为实验效果"></p>
<h2>实验结果</h2>
<p>预处理：筛选长度&lt;=70且长度比在1/3~3范围内的句对</p>
<p>参数：</p>
<ul>
<li>Bi-Transformer中每个模块的层数为2</li>
<li>feed-forward层的大小为512</li>
<li>8 head self-attention</li>
</ul>
<p><img src="wmt17-sentence-level.png" alt="WMT17中sentence-level QE的结果"></p>
<p><img src="wmt18-sentence-level.png" alt="WMT18中sentence-level QE的结果"></p>
<p><img src="wmt-17-18-word-level.png" alt="WMT17/18中word-level QE的结果"></p>
<p>总的来说这一模型大获全胜。</p>
<p>在实验中，即使只留下mis-match feature（而去掉另外两种），结果仍然是比较好的（皮尔森相关系数r降低了约0.04）。（这不禁令人怀疑QE和MT到底有什么区别。）</p>

      
    </div>
    
      <div class="full-width auto-padding tags">
        
          <a href="/tags/Natural-Language-Processing/"><i class="fas fa-hashtag fa-fw"></i>Natural Language Processing</a>
        
          <a href="/tags/Reading-Report/"><i class="fas fa-hashtag fa-fw"></i>Reading Report</a>
        
          <a href="/tags/Machine-Translation/"><i class="fas fa-hashtag fa-fw"></i>Machine Translation</a>
        
          <a href="/tags/Paper/"><i class="fas fa-hashtag fa-fw"></i>Paper</a>
        
          <a href="/tags/Quality-Estimation/"><i class="fas fa-hashtag fa-fw"></i>Quality Estimation</a>
        
      </div>
    
  </section>
</article>

            </div>
          
        
          
            <div class='post-wrapper'>
              <article class="post reveal ">
  
<section class='meta'>
  
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/post/a-simple-fast-and-effective-reparameterization-of-ibm-model-2/">
              
                  论文：A Simple, Fast and Effective Reparameterization of IBM Model 2
              
          </a>
      </h2>
    

    <div class='new-meta-box'>
      
        <div class='new-meta-item author'>
          <a href="https://zhanghuimeng.github.io">
            <i class="fas fa-user" aria-hidden="true"></i>
            张慕晖
          </a>
        </div>
      
      
        <div class="new-meta-item date">
          <a class='notlink'>
            <i class="fas fa-calendar-alt" aria-hidden="true"></i>
            2018-10-31
          </a>
        </div>
      
      
        
      
      
      
    </div>
    <hr>
  </div>
</section>

  <section class="article typo">
    <div class="article-entry" itemprop="articleBody">
      <p>论文地址：<a href="http://www.aclweb.org/anthology/N13-1073" target="_blank" rel="noopener">http://www.aclweb.org/anthology/N13-1073</a></p>
<p>这篇文章对IBM Model 2，一种在无监督情况下将双语语料进行对齐的方法进行了改进，提高了计算速度和对齐质量。</p>
<h2>IBM Model 2</h2>
<p>文章本身很短，但是为了明白文章在讲什么，首先需要搞懂IBM Model 2是什么。<a href="http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/ibm12.pdf" target="_blank" rel="noopener">这篇文章</a>用Fr-En翻译的例子说明了IBM Model 2的定义和训练方法。</p>
<p>用$(f_1, ..., f_m)$表示源句中的$m$个词，$(e_1, ..., e_l)$表示译句中的$l$个词，用$(f^{(k)}, e^{(k)})$表示第$k$个句对。</p>
<p>IBM Model 2是一种Noisy-Channel Approach，也就是说，它的模型分成以下两个部分：</p>
<ul>
<li>语言模型：$p(e)$，表示英语句子$e$在英语中出现的概率</li>
<li>翻译模型：$p(f | e)$，表示在给定译句为$e$的条件下，源句为$f$的概率。</li>
</ul>
<p>有趣的一点是，翻译模型是$p(f | e)$而非$p(e | f)$。（我也不知道为什么）</p>
<h3>对齐模型（alignment model）</h3>
<p>在翻译模型$p(f | e)$中加入对齐变量（alignment variables）$a_1, ..., a_m \in {0, 1, ..., l}$，得到$p(f_1 ... f_m, a_1 ... a_m | e_1 ...e_l, m)$。（上述模型假设已知分布$p(m | l)$，因此把$m$看作是定值）。其中$a_j$表示源句中的$f_j$与译句中的$e_{a_j}$对齐，或者说在概率模型中，$f_j$是由$e_{a_j}$生成的；$e_0$表示NULL，如果$a_j = 0$，说明$f_j$是由NULL生成的。</p>
<p>（上述“生成”说的是概率模型的生成，并不是实际翻译中的生成。）</p>
<h3>IBM-M2模型的正式定义</h3>
<p>一个IBM-M2模型包括一个固定的英语词集合$\mathcal{E}$，一个固定的法语词集合$\mathcal{F}$，以及$M$和$L$，分别表示法语和英语句子的最大长度。模型的参数如下：</p>
<ul>
<li>$t(f | e), f \in \mathcal{F}, e \in \mathcal{E} \cup {\text{NULL}}$：表示从英语词$e$生成法语词$f$的概率</li>
<li>$q(j | i, l, m), l \in {1, ..., L}, m \in {1, ..., M}, i \in {1, ..., n}, j \in {0, ..., l}$：英语和法语句子的长度分别为$l$和$m$时，$a_i = j$（$f_i$与$e_j$对齐）的概率</li>
</ul>
<p>对于任意英语句子$e_1, ..., e_l$和长度$m$，定义法语句子$f_1, ..., f_m$和对齐变量$a_1, ..., a_m$的条件分布为</p>
<p>$$p(f_1 ... f_m, a_1 ... a_m | e_1 ... e_l, m) = \prod_{i=1}^{m} q(a_i | i, l, m) t(f_i | e_{a_i})$$</p>
<hr>
<p>在这个模型中我们使用了一些独立性假设。令$L$为表示英语句子长度的随机变量，$E_1, ..., E_l$是表示英语句子中的词的随机变量；$M$是表示法语句子长度的随机变量，$F_1, ..., F_m$和$A_1, ..., A_m$是表示法语句子中的词和对齐的随机变量，则我们的目标是建立一个这样的模型：</p>
<p>$$P(F_1=f_1 ... F_m=f_m, A_1=a_1 ... A_m=a_m | E_1=e_1 ... E_l=e_l, L=l, M=m)$$<br>
可以通过链式法则把上式分解成两项的乘积：</p>
<ul>
<li>$P(A_1=a_1 ... A_m=a_m | E_1=e_1 ... E_l=e_l, L=l, M=m)$</li>
<li>$P(F_1=f_1 ... F_m=f_m | A_1=a_1 ... A_m=a_m, E_1=e_1 ... E_l=e_l, L=l, M=m)$</li>
</ul>
<p>对于第一项，作如下独立性假设：</p>
<p>$$P(A_1=a_1 ... A_m=a_m | E_1=e_1 ... E_l=e_l, L=l, M=m) = \prod_{i=1}^{m} P(A_i=a_i | L=l, M=m)$$</p>
<p>即对齐变量$A_i$只与源句和译句的长度有关，和具体的词无关。（显然这只是一种假设）</p>
<p>对于第二项，作如下独立性假设：</p>
<p>$$P(F_1=f_1 ... F_m=f_m | A_1=a_1 ... A_m=a_m, E_1=e_1 ... E_l=e_l, L=l, M=m) = \prod_{i=1}^{m} P(F_i=f_i | E_{a_i} = e_{a_i})$$</p>
<p>即法语词$F_i$只与和它对齐的英语词$E_{a_i}$有关，和其他词均无关。（显然这只是一种假设）</p>
<h3>IBM-M2模型的用途</h3>
<ol>
<li>翻译：$\arg\max_{e} p(e) p(f|e)$</li>
<li>语言（词法）模型：$t(f | e)$</li>
<li>对齐模型：$a_i = \arg\max_{j\in{0, ..., l}} (q(j | i, l, m) \times t(f_i | e_j))$</li>
</ol>
<p>其中对齐模型是比较重要的一种用途。</p>
<h3>估计IBM-M2模型的参数</h3>
<h4>数据观测充分的情况</h4>
<p>在这种情况下，我们假设训练数据是这样的：$(f^{(k)}, e^{(k)}, a^{(k)})$。（一般来说，$a^{(k)}$是观察不到的）</p>
<p>这样就可以利用极大似然对参数进行估计：</p>
<ul>
<li>$c(e, f)$：训练数据中英语词$e$与法语词$f$对齐的次数</li>
<li>$c(e)$：训练数据中英语词$e$与任意法语词对齐的总次数</li>
<li>$c(j | i, l, m)$：长度为$l$的法语句子中第$i$个词与长度为$m$的英语句子中第$j$个词对齐的次数</li>
<li>$c(i, l, m)$：训练数据中法语句子长度为$l$，英语句子长度为$m$的总数</li>
</ul>
<p>$$t_{ML} (f | e) = \frac{c(e, f)}{c(e)}$$</p>
<p>$$q_{ML} (j | i, l, m) = \frac{c(j | i, l, m)}{c(i, l, m)}$$</p>
<p><img src="algorithm-1.png" alt="算法伪代码"></p>
<h4>数据观测不充分的情况</h4>
<p>在这种情况下，我们假设训练数据是这样的：$(f^{(k)}, e^{(k)})$。我们采用EM算法对参数进行估计：</p>
<ul>
<li>首先为各$t_{ML} (f | e)$和$q_{ML} (j | i, l, m)$参数估计一个初始值（比如随机取值）</li>
<li>按照当前参数取值统计$c(e, f)$、$c(e)$、$c(j | i, l, m)$和$c(i, l, m)$的值</li>
<li>更新各$t_{ML} (f | e)$和$q_{ML} (j | i, l, m)$参数，迭代直到收敛</li>
</ul>
<p>其中最主要的区别是把原来的</p>
<p>$$\delta(k, i, j) = 1 , \text{if} , a_i^{(k)} = j, 0 , \text{otherwise}$$（只有确实对齐时才取1，其他时候取0）</p>
<p>换成了</p>
<p>$$\delta(k, i, j) = \frac{q(j | i, l_k, m_k) t(f_i^{(k)} | e_j^{(k)})}{\sum_{j=0}^{l_k} q(j | i, l_k, m_k) t(f_i^{(k)} | e_j^{(k)})}$$（在当前参数下$a_i=j$的概率）</p>
<p>可以证明EM算法是收敛的，但是可能会收敛到局部极小值。所以可以用Model 1对参数进行初始化。详细的内容略。</p>
<p><img src="algorithm-2.png" alt="算法伪代码"></p>
<h2>fast align</h2>
<p>这一模型采用了IBM-M2模型的基本思路，但是减少了其中的参数个数。按论文中的表示法，这个模型是这样定义的：</p>
<ul>
<li>令$n$表示源句长度，$m$表示译句长度，定义两个新的参数$p_0$和$\lambda$</li>
<li>令$h(i, j, m, n) = -|\frac{i}{m} - \frac{j}{n}|$</li>
<li>令$\delta(a_i = j | i, m, n)$表示源句和译句长度分别为$m$和$n$时，$a_i = j$的概率（相当于之前的$q(j | i, l, m)$）</li>
<li>令$\theta(e_i | f_{a_i})$表示从英语词$e_i$生成法语词$f_{a_i}$的概率（相当于之前的$t(f | e)$）</li>
</ul>

$$\delta(a_i = j | i, m, n) =
\begin{cases}
p_0 & j = 0 \\
(1 - p_0) \times \frac{e^{\lambda h(i, j, m, n)}}{Z_{\lambda}(i, m, n)} & 0 < j \leq n \\
0 & \text{otherwise}
\end{cases}
$$

<p>其中$Z_{\lambda}(i, m, n) = \sum_{j'=1}^{n} \exp{\lambda h(i, j', m, n)}$</p>
<p>这相当于是把原来每组$(i, j, m, n)$都对应一个参数的情况修正成了只通过已知函数$h$和$p_0$、$\lambda$两个参数来确定对齐。其中$p_0$表示的是无对齐的概率</p>
<h3>根据参数计算给定句对的最大似然概率和对齐</h3>
<p>给定句对$(f, e)$和参数，则：</p>
<ul>
<li>译句中第$i$个词为$e_i$，且$e_i$和$f_{a_i}$对齐的概率为：$p(e_i, a_i | f, m, n) = \delta(a_i | i, m, n) \times \theta(e_i | f_{a_i})$</li>
<li>译句中第$i$个词为$e_i$的概率为：$p(e_i | f, m, n) = \sum_{j=0}^{n} p(e_i, a_i=j | f, m, n)$</li>
<li>因此：$p(e | f) = \prod_{i=1}^{m} p(e_i, a_i=j | f, m, n) = \prod_{i=1}^{m} \sum_{j=0}^{n} \delta(a_i | i, m, n) \times \theta(e_i | f_{a_i})$</li>
</ul>
<p>通过一些优化手段，我们可以使$\delta(a_i | i, m, n)$的计算复杂度是$O(1)$的。</p>
<hr>
<p>显然$\delta(a_i | i, m, n)$的计算瓶颈在于$Z_{\lambda}(i, m, n)$；$Z_{\lambda}(i, m, n)$显然可以在$O(n)$时间复杂度内进行计算，但事实上可以是$O(1)$的。考虑到$h(i, j, m, n) = -|\frac{i}{m} - \frac{j}{n}|$，事实上$Z_{\lambda}(i, m, n)$是两个等比数列的和：</p>
<ul>
<li>令$j_{\uparrow} = \lfloor \frac{i \times n}{m} \rfloor$，$j_{\downarrow} = j_{\uparrow} + 1$</li>
<li>则$Z_{\lambda}(i, m, n) = \sum_{j'=1}^{j_{\uparrow}} \exp{\lambda h(i, j', m, n)} + \sum_{j' = j_{\downarrow}}^{n} \exp{\lambda h(i, j', m, n)}$</li>
<li>其中$\sum_{j'=1}^{j_{\uparrow}} \exp{\lambda h(i, j', m, n)} = e^{\lambda j_{\uparrow}} + e^{\lambda (j_{\uparrow} - 1/n)} + e^{\lambda (j_{\uparrow} - 2/n)} + ...$</li>
<li>$\sum_{j' = j_{\downarrow}}^{n} \exp{\lambda h(i, j', m, n)} = e^{\lambda j_{\downarrow}} + e^{\lambda (j_{\downarrow} - 1/n)} + e^{\lambda (j_{\downarrow} - 2/n)} + ...$</li>
</ul>
<p>因此可以在$O(1)$时间内计算$Z_{\lambda}(i, m, n)$。</p>
<h3>根据训练数据计算参数</h3>
<p>这一模型也可以通过EM算法来进行训练。$\theta(e_i | f_{a_i})$的更新方法和之前类似，并使用了（一些我看不懂的数学）进行优化；$\lambda$参数则需要用梯度方法来进行更新（另一些我没看懂的数学）。事实上，$\delta(j | i, m, n)$的导数也具有类似的规律性，因此也可以用相同的方法来计算梯度，复杂度比较低。</p>
<h2>评价</h2>
<p>这个模型能够用无监督且非常快速的方式计算出源句和译句之间的对齐，是一种非常有趣且已经被广泛使用的方法。不过由于模型和方法本身的限制，这个模型的对齐质量仍然不是很高。不过，在NMT方法中我们仍然可以利用这一方法生成的对齐的质量来对训练语料进行初步的预处理。</p>

      
    </div>
    
      <div class="full-width auto-padding tags">
        
          <a href="/tags/Natural-Language-Processing/"><i class="fas fa-hashtag fa-fw"></i>Natural Language Processing</a>
        
          <a href="/tags/Reading-Report/"><i class="fas fa-hashtag fa-fw"></i>Reading Report</a>
        
          <a href="/tags/Machine-Translation/"><i class="fas fa-hashtag fa-fw"></i>Machine Translation</a>
        
          <a href="/tags/Paper/"><i class="fas fa-hashtag fa-fw"></i>Paper</a>
        
      </div>
    
  </section>
</article>

            </div>
          
        
          
            <div class='post-wrapper'>
              <article class="post reveal ">
  
<section class='meta'>
  
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/post/findings-of-the-wmt-2018-shared-task-on-quality-estimation/">
              
                  论文：Findings of the WMT 2018 Shared Task on Quality Estimation
              
          </a>
      </h2>
    

    <div class='new-meta-box'>
      
        <div class='new-meta-item author'>
          <a href="https://zhanghuimeng.github.io">
            <i class="fas fa-user" aria-hidden="true"></i>
            张慕晖
          </a>
        </div>
      
      
        <div class="new-meta-item date">
          <a class='notlink'>
            <i class="fas fa-calendar-alt" aria-hidden="true"></i>
            2018-10-29
          </a>
        </div>
      
      
        
      
      
      
    </div>
    <hr>
  </div>
</section>

  <section class="article typo">
    <div class="article-entry" itemprop="articleBody">
      <p>论文地址：<a href="https://fredblain.org/papers/pdf/specia_et_al_findings_of_the_wmt_2018_shared_task_on_quality_estimation.pdf" target="_blank" rel="noopener">Findings of the WMT 2018 Shared Task on Quality Estimation</a></p>
<p>这篇文章报告了WMT18 Quality Estimation任务的结果。</p>
<h2>各系统及提交者</h2>
<p>本次参与评测的一共有10个系统（其中UAlacant和RTM因为迟交没有计入正式排名）：</p>
<table>
<thead>
<tr>
<th style="text-align:center">系统名称</th>
<th style="text-align:center">提交者</th>
<th style="text-align:center">具体提交</th>
<th style="text-align:center">参与任务</th>
<th style="text-align:center">Task1</th>
<th style="text-align:center">Task2</th>
<th style="text-align:center">Task3</th>
<th style="text-align:center">Task4</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">CMU-LTI</td>
<td style="text-align:center">CMU</td>
<td style="text-align:center">1. CMU-LTI</td>
<td style="text-align:center">T2</td>
<td style="text-align:center">-</td>
<td style="text-align:center">次于QEBrain，和SHEF-PT表现相当</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
</tr>
<tr>
<td style="text-align:center">JU-USAAR</td>
<td style="text-align:center">Jadavpur University &amp; University of Saarland</td>
<td style="text-align:center">1. Bag-of-Words<br>2. Doc2Vec</td>
<td style="text-align:center">T2</td>
<td style="text-align:center">-</td>
<td style="text-align:center">低于baseline</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
</tr>
<tr>
<td style="text-align:center">MQE</td>
<td style="text-align:center">Vicomtech</td>
<td style="text-align:center">1. sMQE<br>2. uMQE</td>
<td style="text-align:center">T1</td>
<td style="text-align:center">和baseline持平</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
</tr>
<tr>
<td style="text-align:center">QEbrain</td>
<td style="text-align:center">阿里</td>
<td style="text-align:center">1. QEBrain DoubleBi w/ BPE+word-tok<br>2. QEBrain DoubleBi w/ BPE+word-tok (ensemble)</td>
<td style="text-align:center">T1, T2</td>
<td style="text-align:center">QEBrain和UNQE并列第一，远高于其他系统</td>
<td style="text-align:center">第一名，且gap error detection子任务表现非常好</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
</tr>
<tr>
<td style="text-align:center">RTM</td>
<td style="text-align:center">Biçici</td>
<td style="text-align:center">1. RTM<br>2. RTM_MIX1<br>3. RTM_MIX5<br>4. RTM_MIX6<br>5. RTM_MIX7</td>
<td style="text-align:center">T1-T4</td>
<td style="text-align:center">在NMT数据集上接近UNQE和QEBrain的结果</td>
<td style="text-align:center">低于baseline</td>
<td style="text-align:center">低于baseline</td>
<td style="text-align:center">远不如baseline</td>
</tr>
<tr>
<td style="text-align:center">SHEF</td>
<td style="text-align:center">University of Sheffield</td>
<td style="text-align:center">1. SHEF-PT<br>2. SHEF-bRNN<br>3. SHEF-ATT-SUM<br>4. SHEF-PT-indomain<br>5. SHEF-mtl-bRNN<br>6. SHEF-mtl-PT-indomain</td>
<td style="text-align:center">T1-T4</td>
<td style="text-align:center">大致处于排名中间位置</td>
<td style="text-align:center">第二名（因为很多系统没有参与Task2）</td>
<td style="text-align:center">与baseline相当</td>
<td style="text-align:center">略高于baseline</td>
</tr>
<tr>
<td style="text-align:center">TSKQE</td>
<td style="text-align:center">汉堡大学</td>
<td style="text-align:center">1. TSKQE1<br>2. TSKQE2<br></td>
<td style="text-align:center">T1</td>
<td style="text-align:center">第三名（低于QEBrain和UNQE）</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
</tr>
<tr>
<td style="text-align:center">UAlacant</td>
<td style="text-align:center">University of Alacant</td>
<td style="text-align:center">1. UAlacant</td>
<td style="text-align:center">T1, T2</td>
<td style="text-align:center">和baseline相当</td>
<td style="text-align:center">低于baseline</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">UNQE</td>
<td style="text-align:center">江西师范大学</td>
<td style="text-align:center">UNQE</td>
<td style="text-align:center">T1</td>
<td style="text-align:center">和QEBrain并列第一，远高于其他系统</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
</tr>
<tr>
<td style="text-align:center">UTaru</td>
<td style="text-align:center">University of Taru</td>
<td style="text-align:center">1. UTartu/QuEst+Attention<br>2. UTartu/QuEst+Att+CrEmb3</td>
<td style="text-align:center">T1, T2</td>
<td style="text-align:center">略高于baseline</td>
<td style="text-align:center">低于baseline</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
</tr>
</tbody>
</table>
<p>可以看出，这次比赛中，QEBrain和UNQE系统的效果是最好的，其次是CMU-LTI、TSKQE和SHEF。</p>
<h3>QEBrain（T1，T2，阿里）</h3>
<p>QEBrain系统的参赛者已经把论文贴到Arxiv上了（<a href="https://arxiv.org/pdf/1807.09433.pdf" target="_blank" rel="noopener">“Bilingual Expert” Can Find Translation Errors</a>），之后我会再去仔细读……不过简单来说好像是这样的：</p>
<ul>
<li>feature extraction (target language) model（作为特征提取器）
<ul>
<li>multi-head self-attention</li>
<li>源语言的transformer encoder</li>
<li>目标语言的bi-transformer encoder</li>
</ul>
</li>
<li>预训练过的双向transformer（作为预测器）
<ul>
<li>输入上述特征提取器得到的特征和baseline系统提供的特征</li>
</ul>
</li>
</ul>
<p>并且进行了ensemble。</p>
<h3>UNQE（T1，江西师范大学）</h3>
<p>他们把论文投到IEICE了（<a href="https://www.jstage.jst.go.jp/article/transinf/E101.D/9/E101.D_2018EDL8019/_article/-char/en" target="_blank" rel="noopener">A Unified Neural Network for Quality Estimation of Machine Estimation</a>），之后我会再去仔细读……不过简单来说好像是这样的：</p>
<ul>
<li>bi-RNN encoder-decoder + attention：从翻译输出中提取quality vector</li>
<li>RNN：通过quality vector预测翻译输出的HTER值</li>
</ul>
<p>模型进行了预训练；输出结果进行了模型平均。</p>
<h3>CMU-LTI（T2，CMU）</h3>
<p>他们也把论文贴到arXiv了（<a href="https://arxiv.org/pdf/1809.00129.pdf" target="_blank" rel="noopener">Contextual Encoding for Translation Quality Estimation</a>）。模型分为三个主要部分：</p>
<ul>
<li>embedding layer：表示词和POS tag</li>
<li>1d convolution layer：将每个词和它的local context结合起来</li>
<li>stack of feed-forward and RNN: 将每个词和它的global context结合起来；同时输入一些句法feature</li>
</ul>
<h3>TSKQE（T1，汉堡大学）</h3>
<p>我确实没找到他们今年的论文（只找到了去年的，<a href="http://www.statmt.org/wmt17/pdf/WMT62.pdf" target="_blank" rel="noopener">UHH Submission to the WMT17 Quality Estimation Shared Task</a>）；他们的做法好像是对源句应用sequence kernel、tree kernel，对译句应用candidate translation和back-translation，预测HTER得分。（我并没有看懂这些，好像是一种非深度学习的approach）</p>
<h3>SHEF（T1-T4，University of Sheffield）</h3>
<p>唯一在四个任务上都有正式提交的系统。我仍然没有找到他们今年的论文，但我找到了他们的一个类似的<a href="https://sheffieldnlp.github.io/deepQuest/introduction.html" target="_blank" rel="noopener">开源项目</a>。今年他们提交了两个不同架构的系统：</p>
<ul>
<li>SHEF-PT：由Predictor和Estimator组成
<ul>
<li>Predictor：经过一定修改的encoder-decoder RNN模型</li>
<li>Estimator：bi-RNN，基于Predictor的输出进行预测</li>
<li>可以进行multi-task learning</li>
</ul>
</li>
<li>SHEF-bRNN：
<ul>
<li>用两个bi-RNN（GRU）学习(source, translation)对，输出word-level的预测</li>
<li>用attention机制对bi-RNN的输出（word-level的预测）进行加权平均，得到sentence-level的预测</li>
</ul>
</li>
</ul>
<p>对于phrase-level的预测，他们使用的是标准的基于attention的MT架构，将word-level的预测加权平均得到结果；对于预测source tag的任务，是把两边的输出反过来；对于document-level的任务，同时使用了PT和bRNN两种架构。</p>
<hr>
<p>剩余的分数不太高的系统就不仔细看了。感觉NN方法占据了绝对优势。</p>
<h2>Task1（句级QE）结果及讨论</h2>
<p>这一任务的目标是对翻译输出的质量进行打分或排名。训练数据的label包括HTER、post-editing时间和post-editing中键盘敲击统计。</p>
<p>评价方法主要是皮尔森相关系数r（打分）和斯皮尔曼等级相关系数（排名）。</p>
<p>baseline是<a href="https://github.com/ghpaetzold/questplusplus" target="_blank" rel="noopener">QUEST++</a>：提取feature，用SVR+RBF kernel进行回归训练。</p>
<p><img src="task1-en-ge-result.png" alt="En-De数据集测试结果"></p>
<p><img src="task1-ge-en-result.png" alt="De-En数据集测试结果"></p>
<p><img src="task1-en-lv-result.png" alt="En-Lv数据集测试结果"></p>
<p><img src="task1-en-cz-result.png" alt="En-Cs数据集测试结果"></p>
<p>在该任务上表现最好的系统显然是QEBrain和UNQE，且它们的表现都远好于第三名；其中SHEF-PT是去年在该任务上表现最好的。这说明了Transformer技术应用在QE任务上之后大大提高了QE的表现。</p>
<p>SMT和NMT数据集的生成方法相同（分别用SMT和NMT系统对一个初始数据集进行翻译，进行post-edit，再移除其中过多的HTER=0的句对），但由于NMT的翻译效果远优于SMT，导致NMT数据集比较小，这也使得我们无法直接对SMT和NMT数据集上的结果进行比较；同时，NMT数据集上的平均HTER分数也更低，这些可能是导致En-De数据集上各系统普遍在NMT数据上表现较差的原因。但是在En-Lv数据集上，这一趋势完全是相反的，NMT数据上系统的表现较好。不过系统在不同数据集上的排名是类似的，说明QE系统一般具有鲁棒性。</p>
<p>另一个事实是，没有系统使用了post-editing时间和post-editing中键盘敲击统计这两种label。</p>
<h2>Task2（词级QE）结果及讨论</h2>
<p>这个任务相当于有三个子任务，分别对三种不同的token进行标记，并分别进行测试：</p>
<ul>
<li>译句中的普通token：根据post-edited版本进行标注，应被替换和删除的标记为BAD，其余标为OK</li>
<li>译句中的gap token：在译句中每个token之后和句首插入gap token（也就是说，如果原来有N个token，插入之后会变成2*N+1个token），如果gap token对应位置相比post-edited版本发生漏词，则该gap token标记为BAD；否则标为OK</li>
<li>源句中的token：使用fastalign工具将源句和post-edited版本的译句进行对齐；对于源句中的每个token，如果和它对齐的post-edited版本的译句中的token在译句中被删除或替换，则该token标记为BAD；否则标为OK</li>
</ul>
<p>评价方式：对上述每种token的OK和BAD类别分别计算F1分数并相乘，以F1-mult作为最终评分标准。</p>
<p>baseline：提取feature后，作为sequence prediction问题，用CRF算法进行训练。</p>
<p><img src="task2-en-ge-result.png" alt="En-De数据集测试结果"></p>
<p><img src="task2-ge-en-result.png" alt="De-En数据集测试结果"></p>
<p><img src="task2-en-cz-result.png" alt="En-Cs数据集测试结果"></p>
<p><img src="task2-en-lv-result.png" alt="En-Lv数据集测试结果"></p>
<p>由于在各个数据集上进行提交的系统都不太一样，所以很难进行完整的比较。</p>
<p>参与En-De和De-En任务的系统最多；和往年一样，Task1的结果和Task2的结果相关性很强，所以QEBrain在task2也获胜了。由于UNQE等系统没有参与Task2，表现次好的系统是SHEF-PT，落后得稍微少一点。</p>
<p>对于En-De数据集，很显然各个系统在NMT数据集上的表现远差于SMT数据集；而En-Lv数据集中，系统在SMT数据集上表现更好，这和Task1也相同。</p>
<p>只有很少几个系统提交了gap检测和导致错误的源词的新任务；这些系统的表现都比较一般，但结果和在主任务上的表现是相关的。QEBrain系统在gap检测上的表现非常之好（没有在其他任务上提交）。并且从分数可以看出，预测源句中导致错误的token比预测译句中的错误token更难；这可能是因为“源句中导致错误的token”有更多的可能性。</p>
<p>En-Lv NMT数据集上，所有系统都只能达到baseline的效果；En-Cs数据集上所有系统的表现都差不多。这可能是因为预处理数据资源不够。</p>
<h2>Task3（短语级QE）结果及讨论</h2>
<p>短语有四类标注：</p>
<ul>
<li>OK：正确的短语</li>
<li>BAD：包含错误的短语</li>
<li>BAD_word_order：短语在句中位于错误的位置</li>
<li>BAD_omission：短语前后缺词</li>
</ul>
<p>这一任务分成两个子任务：</p>
<ul>
<li>Task3a：和Task2一样，在译句中每个token之后和句首插入gap token；然后用SMT decoder将译句分成短语，对短语进行标注：最后将标注结果标到具体的词上，相当于这也是一个word-level的任务</li>
<li>Task3b：直接对短语进行标注，每个短语前后各有一个gap token，gap可能被标注为OK或BAD_omission。</li>
</ul>
<p>评价方式：</p>
<ul>
<li>Task3a：和Task 2相同，都是OK和BAD类别下的F1-mult</li>
<li>Task3b：短语级别的F1-mult</li>
</ul>
<p>这一任务只有De-En数据集，数据是手动标注的（而非post-edit后再自动生成的）。</p>
<p>baseline：提取feature后，作为sequence labelling问题，用CRF算法进行训练。</p>
<p><img src="task3-ge-en-result.png" alt="De-En数据集测试结果"></p>
<p>和Task2的De-En结果相比，BAD类别的F1分数显著降低了。这一现象可能是因为数据生成的方式导致的，phrase label粒度更粗一些。</p>
<p>事实上，只有SHEF-PT和SHEF-bRNN系统参与了这一任务；它们在Task3a上的表现和baseline差不多，在Task3b上还不如baseline。作者据此认为短语级别的预测仍然是很有挑战的任务，但我觉得样本量太少了，也不是很有代表性。不过这可能说明我们需要更好的神经网络结构。</p>
<h2>Task4（文档级QE）结果及讨论</h2>
<p>这一任务的要求是对整个文档的翻译质量进行打分。文档中在accuracy、fluency和style三个方面出错的词被标出，并根据严重程度分为minor、major和critical三类；文档的真实得分由人类打出。</p>
<p>评价方式是预测得分和真实得分的皮尔森相关系数r。</p>
<p>baseline：<a href="https://github.com/ghpaetzold/questplusplus" target="_blank" rel="noopener">QUEST++</a></p>
<p><img src="task4-en-fr-result.png" alt="En-Fr数据集测试结果"></p>
<p>显然，baseline的得分已经很高了；只有SHEf-PT-indomain的分数稍高于baseline。这说明Task4是一项很难的任务；同时很难评价系统在这一任务上的表现。</p>

      
    </div>
    
      <div class="full-width auto-padding tags">
        
          <a href="/tags/Natural-Language-Processing/"><i class="fas fa-hashtag fa-fw"></i>Natural Language Processing</a>
        
          <a href="/tags/Reading-Report/"><i class="fas fa-hashtag fa-fw"></i>Reading Report</a>
        
          <a href="/tags/Machine-Translation/"><i class="fas fa-hashtag fa-fw"></i>Machine Translation</a>
        
          <a href="/tags/Paper/"><i class="fas fa-hashtag fa-fw"></i>Paper</a>
        
          <a href="/tags/Quality-Estimation/"><i class="fas fa-hashtag fa-fw"></i>Quality Estimation</a>
        
      </div>
    
  </section>
</article>

            </div>
          
        
          
            <div class='post-wrapper'>
              <article class="post reveal ">
  
<section class='meta'>
  
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/post/learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation/">
              
                  论文：Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation
              
          </a>
      </h2>
    

    <div class='new-meta-box'>
      
        <div class='new-meta-item author'>
          <a href="https://zhanghuimeng.github.io">
            <i class="fas fa-user" aria-hidden="true"></i>
            张慕晖
          </a>
        </div>
      
      
        <div class="new-meta-item date">
          <a class='notlink'>
            <i class="fas fa-calendar-alt" aria-hidden="true"></i>
            2018-10-03
          </a>
        </div>
      
      
        
      
      
      
    </div>
    <hr>
  </div>
</section>

  <section class="article typo">
    <div class="article-entry" itemprop="articleBody">
      <p>论文地址：<a href="https://arxiv.org/abs/1406.1078" target="_blank" rel="noopener">https://arxiv.org/abs/1406.1078</a></p>
<p>这篇文章提出了RNN Encoder-Decoder架构，使得RNN能够处理序列数据的输入输出：先把序列数据encode成一个定长vector，再把它decode成另一个序列。有趣的一点是，这篇文章的题目里带了“SMT”这个词，说明它并不是一种纯NMT的方法——事实上论文里用它替代了现存的方法里给短语打分的部分。当然这种方法也是可以直接用于整句翻译的（<a href="https://arxiv.org/abs/1409.1259" target="_blank" rel="noopener">On the Properties of Neural Machine Translation: Encoder-Decoder Approaches</a>），但由于RNN的特性，使得在长句上表现不太好，最后又改进出了<a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener">Attention方法</a>。<del>目前我还不知道这篇文章和Seq2Seq具体是什么关系。</del></p>
<p>2018.10.11 UPDATE：Seq2Seq和这篇文章提出的架构很类似，但是提高了长句翻译的表现（通过把句子倒过来的trick），一般说Seq2Seq架构的时候应该指的是那篇文章（至少我认为是这样）。本文的另一个重要贡献是LSTM的简化版，<a href="https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/GRUCell" target="_blank" rel="noopener">GRU单元</a>。</p>
<h2>简介</h2>
<p>本文中提出了一种新的模型，称为RNN Encoder-Decoder，包括两个RNN。一个RNN（encoder）把符号序列编码成一个定长向量表示（fixed-length vector representation）；另一个RNN（decoder）把该表示解码成另一个符号序列。这两个RNN共同被训练，以最大化输出目标序列的概率。我们同时提出了一种新的隐藏层单元（hidden unit）。将该模型计算出的短语对条件概率作为现有SMT模型的额外特征之后，SMT的翻译结果提升了；且可以发现，该模型学到的短语中间表示在语义上和句法上都是有意义的。</p>
<h2>RNN</h2>
<p>RNN是一个神经网络，它输入变长序列$\mathbf{x} = (x_1, ..., x_T)$，内部有一个隐状态$\mathbf{h}$，输出（可选）为$\mathbf{y}$。在每个时刻$t$，RNN的隐状态$\mathbf{h}_{\langle t \rangle}$会被更新：</p>
<p>$$\mathbf{h}_{\langle t \rangle} = f(\mathbf{h}_{\langle t - 1 \rangle}, x_t)$$</p>
<p>其中$f$是一个非线性激活函数，可能很简单，也可能很复杂（如LSTM）。</p>
<p>RNN可以通过被训练为预测序列中的下一个符号来学习序列的概率分布。在这种情况下，$t$时刻输出的就是概率分布$p(x_t | x_{t-1}, ..., x_1)$。比如说，一个multinomial distribution（1-K编码）就可以用一个softmax激活函数输出（这里并没有看懂……）：</p>
<p>$$p(x_{t, j} = 1 | x_{t-1}, ..., x_1) = \frac{\exp{(\mathbf{w}_j\mathbf{h}_{\langle t \rangle})}}{\sum_{j'=1}^{K} \exp{(\mathbf{w}_{j'}\mathbf{h}_{\langle t \rangle})}}$$</p>
<p>其中$j = 1, ..., K$，$\mathbf{w}_j$是权重矩阵$\mathbf{W}$的行。</p>
<p>现在就可以计算出序列$\mathbf{x}$出现的概率了：</p>
<p>$$p(\mathbf{x}) = \prod_{t=1}^T p(x_t | x_{t-1}, ..., x_1)$$</p>
<p>通过这一学到的分布，生成一个新的序列的方法是显然的，逐步选择符号即可。</p>
<h2>RNN Encoder-Decoder</h2>
<p>之前已经说过了，RNN Encoder-Decoder是把一个变长序列编码为一个定长向量表示，再把这个表示解码为另一个变长序列的过程。从概率论的角度看（但是我不知道为什么要从概率论的角度看），这是学习两个变长序列之间的条件概率的方法：</p>
<p>$$p(y_1, ..., y_{T'} | x_1, ..., x_T)$$</p>
<h3>Encoder</h3>
<p>Encoder是一个RNN，它顺序读入输入序列$\mathbf{x}$，并逐步更新隐状态（和普通的RNN是一样的）：</p>
<p>$$\mathbf{h}_{\langle t \rangle} = f(\mathbf{h}_{\langle t - 1 \rangle}, x_t)$$</p>
<p>读到序列结尾（EOS）之后，RNN的隐状态就是整个输入序列对应的表示$\mathbf{c}$。</p>
<h3>Decoder</h3>
<p>Decoder也是一个RNN，它通过隐状态$\mathbf{h}_{\langle t \rangle}$预测下一个符号$y_t$。不过，$y_t$和$\mathbf{h}_{\langle t \rangle}$都依赖于$y_{t-1}$和$\mathbf{c}$，所以$t$时刻的隐状态为：</p>
<p>$$\mathbf{h}_{\langle t \rangle} = f(\mathbf{h}_{\langle t - 1 \rangle}, y_{t-1}, \mathbf{c})$$</p>
<p>相似地，下一个符号的条件分布就是（虽然不是很懂这是怎么相似出来的）：</p>
<p>$$P(y_t | y_{t-1}, y_{t-2}, ..., y_1, \mathbf{c}) = g(\mathbf{h}_{\langle t \rangle}, y_{t-1}, \mathbf{c})$$</p>
<h3>Encoder+Decoder</h3>
<p><img src="encdec.png" alt="RNN Encoder-Decoder图示"></p>
<p>Encoder和Decoder共同进行训练，以最大化conditional log-likelihood：</p>
<p>$$\max_{\mathbf{\theta}} \frac{1}{N} \sum_{n=1}^N \log{p_{\mathbf{\theta}}(\mathbf{y}_n | \mathbf{x}_n)}$$</p>
<p>其中$\mathbf{\theta}$是模型参数，每个$(\mathbf{x}_n, \mathbf{y}_n)$都是训练集中的一个输入输出对。由于decoder的输出是可微分的， 因此可以通过基于梯度的算法来估计模型参数。</p>
<p>训练完RNN Encoder-Decoder之后，模型可以通过两种方式使用。一种是根据输入序列来生成输出序列。另一种是对给定的输入输出序列进行打分，分数就是概率$p_{\mathbf{\theta}}(\mathbf{y} | \mathbf{x})$。</p>
<h2>新的隐藏单元</h2>
<p><img src="gru.png" alt="隐藏单元图示"></p>
<p>这一单元的灵感来自LSTM，但是计算和实现都简单得多。图中$z$是update gate，用于控制当前隐状态是否需要被新的隐状态$\tilde{h}$更新；$r$是reset gate，用于确定是否要丢弃上一个隐状态。</p>
<blockquote>
<p>这个计算方法是否说明，是很多个隐藏单元一起更新和训练……但是为什么输入是个向量呢？大概是因为1-K表示法和Embedding？</p>
</blockquote>
<p>2018.10.11 UPDATE：用一般的术语来说，下列内容实际上说明的是“一个GRU cell中的一个unit的计算过程”，因此$r_j$、$z_j$和$h_j^{\langle t \rangle}$都是标量。在本文中，layer=cell。</p>
<p>$r_j$通过下式计算：</p>
<p>$$r_j = \sigma([\mathbf{W}_r\mathbf{x}]_j + [\mathbf{U}_r \mathbf{h}_{\langle t-1 \rangle}]_j)$$</p>
<p>其中$\sigma$是<a href="https://en.wikipedia.org/wiki/Logistic_function" target="_blank" rel="noopener">Logistic Sigmoid</a>函数，$[.]_j$是向量的第$j$个元素，$\mathbf{x}$是输入，$\mathbf{h}_{\langle t-1 \rangle}$是上一个隐状态，$\mathbf{W}_r$和$\mathbf{U}_r$是学习到的权重矩阵。</p>
<p>$z_j$类似地通过下式计算：</p>
<p>$$z_j = \sigma([\mathbf{W}_z\mathbf{x}]_j + [\mathbf{U}_z \mathbf{h}_{\langle t-1 \rangle}]_j)$$</p>
<p>单元$h_j$的实际激活状态通过下式计算：</p>
<p>$$h_j^{\langle t \rangle} = z_j h_j^{\langle t-1 \rangle} + (1 - z_j) \tilde{h}_j^{\langle t \rangle}$$</p>
<p>其中</p>
<p>$$\tilde{h}_j^{\langle t \rangle} = \phi([\mathbf{W}\mathbf{x}]_j + [\mathbf{U}(\mathbf{r} \odot \mathbf{h}_{\langle t-1 \rangle})]_j)$$</p>
<p><del>（虽然我看不懂这个式子是怎么使用$r_j$的，以及它对激活状态有什么影响……）</del>reset gate通过与$\mathbf{h}_{\langle t-1 \rangle})$点乘对$\tilde{h}_j^{\langle t \rangle}$产生影响。</p>
<hr>
<h3>另一种对GRU的描述方式</h3>
<h2>SMT模型和RNN Encoder-Decoder的结合</h2>
<p>传统的SMT系统的目标是对于源句$\mathbf{e}$，找到一个使下式最大化的翻译$\mathbf{f}$：</p>
<p>$$p(\mathbf{f} | \mathbf{e}) \propto p(\mathbf{e} | \mathbf{f}) p(\mathbf{f})$$</p>
<p>其中$p(\mathbf{e} | \mathbf{f})$称为翻译模型（translation model），$p(\mathbf{f})$称为语言模型（language model）。</p>
<p>但在实际中，大部分SMT系统都把$\log{p(\mathbf{f} | \mathbf{e})}$做为一个log-linear模型，包括一些额外的feature和相应的权重：</p>
<p>$$\log{p(\mathbf{f} | \mathbf{e})} = \sum_{n=1}^N w_n f_n(\mathbf{f}, \mathbf{e}) + \log{Z(\mathbf{e})}$$</p>
<p>其中$f_n$是feature，$w_n$是权重，$Z(\mathbf{e})$是与权重无关的normalization constant。</p>
<p>在基于短语的SMT模型中，翻译模型$p(\mathbf{e} | \mathbf{f})$被分解为源句和目标句中短语匹配的概率。这一概率再一次被作为log-linear模型中的额外feature进行优化。</p>
<hr>
<p>作者在一个短语对表中训练RNN Encoder-Decoder，并将得到的分数作为log-linear模型中的额外feature。目前的做法是把得到的短语对分数直接加入现有的短语对表中；事实上也可以直接用RNN Encoder-Decoder代替这个表，但这就意味着对于每个源短语，RNN Encoder-Decoder都需要生成一系列好的目标短语，因此需要进行很多采样，这太昂贵了。</p>
<h2>实验</h2>
<p>在WMT'14的En-Fr任务上进行了评测。对于每种语言都只保留了最常见的15000个词，将不常用的词标记为[UNK]。</p>
<p>实验中，RNN Encoder-Decoder的encoder和decoder各有1000个隐藏单元。每个输入符号$x_{\langle t \rangle}$和隐藏单元之间的输入矩阵用两个低秩（100）矩阵来模拟，相当于学习了每个词的100维embedding。隐藏单元中的$\tilde{h}$使用的是双曲余弦函数（hyperbolic tangent function）。decoder中隐状态到输出的计算使用的是一个深度神经网络，含有一个包含了500个maxout单元的中间层。</p>
<p>RNN Encoder-Decoder的权重初值都是通过对一个各向同性的均值为零的高斯分布采样得到的，其标准差为0.01。（但是另一种权重矩阵的初值不一样，而且我没看懂……）</p>
<p>通过Adadelta和随机梯度下降法进行训练，其中超参数为$\epsilon = 10^{-6}$，$\rho = 0.95$。每次更新时，从短语表中随机选出64个短语对。模型训练了大约3天。</p>
<p><img src="table1.png" alt="实验结果"></p>
<p>因为CSLM和RNN Encoder-Decoder共同使用能进一步提高表现，说明这两种方法对结果的贡献并不相同。</p>
<p>除此之外，它学习到的word embedding矩阵也是有意义的。</p>
<p><img src="figure4.png" alt="Word Embedding和词义"></p>
<p>（不过考虑到这就是Word Embedding的根本用途，这件事听起来就没有那么令人兴奋了……）</p>
<h2>附录：RNN Encoder-Decoder的详细描述</h2>
<p>令$X = (\mathbf{x}_1, \mathbf{x}_2, ..., \mathbf{x}_N)$表示源短语，$Y = (\mathbf{y}_1, \mathbf{y}_2, ..., \mathbf{y}_M)$。每个短语都是一系列$K$维的one-hot向量。</p>
<h3>Encoder</h3>
<p>源短语的每个词都被embed成了500维：$e(\mathbf{x}_i) \in \mathbb{R}^{500}$。</p>
<p>encoder的隐状态由1000个隐藏单元组成，其中每一个单元在$t$时刻的状态由下式计算：</p>
<p>$$h_j^{\langle t \rangle} = z_j h_j^{\langle t-1 \rangle} + (1 - z_j) \tilde{h}_j^{\langle t \rangle}$$</p>
<p>其中</p>
<p>$$\tilde{h}_j^{\langle t \rangle} = \tanh([\mathbf{W}e(\mathbf{x}_t)]_j + [\mathbf{U}(\mathbf{r} \odot \mathbf{h}_{\langle t-1 \rangle})]_j)$$</p>
<p>$$z_j = \sigma([\mathbf{W}_z e(\mathbf{x}_t)]_j + [\mathbf{U}_z \mathbf{h}_{\langle t-1 \rangle}]_j)$$</p>
<p>$$r_j = \sigma([\mathbf{W}_r e(\mathbf{x}_t)]_j + [\mathbf{U}_r \mathbf{h}_{\langle t-1 \rangle}]_j)$$</p>
<p>其中$\sigma$是logistic sigmoid函数，$\odot$是元素对应乘积。上式中忽略了偏移项。初始隐状态$h_j^{\langle 0 \rangle} = 0$。</p>
<p>在隐状态计算完第$N$步之后，就可以得到源短语的表示$\mathbf{c}$：</p>
<p>$$\mathbf{c} = \tanh{\mathbf{V}\mathbf{h}^{\langle N \rangle}}$$</p>
<p>（但是$\mathbf{V}$矩阵是哪里来的？也是需要学习的吗？）</p>
<h3>Decoder</h3>
<p>decoder通过下式对隐状态进行初始化：</p>
<p>$$\mathbf{h'}^{\langle 0 \rangle} = \tanh(\mathbf{V'c})$$</p>
<p>（大概$\mathbf{V'}$矩阵也是一个参数吧。当然和Encoder的参数不一样）</p>
<p>decoder的隐藏单元在时刻$t$的隐状态通过下式计算：</p>
<p>$${h'}_j^{\langle t \rangle} = {z'}_j {h'}_j^{\langle t-1 \rangle} + (1 - {z'}_j) \tilde{h'}_j^{\langle t \rangle}$$</p>
<p>其中</p>
<p>$$\tilde{h'}_j^{\langle t \rangle} = \tanh([\mathbf{W'}e(\mathbf{y}_{t-1})]_j + r'_j [\mathbf{U'}\mathbf{h'}_{\langle t-1 \rangle} + \mathbf{Cc}]_j)$$</p>
<p>（我在上式的最后一项上加了个$j$。我觉得可能打错了，虽然更有可能是我看错了，不过也没有找到什么验证的方法。）</p>
<p>$${z'}_j = \sigma([\mathbf{W'}_z e(\mathbf{y}_{t-1})]_j + [\mathbf{U'}_z \mathbf{h'}_{\langle t-1 \rangle}]_j + [\mathbf{C}_z\mathbf{c}]_j)$$</p>
<p>$${r'}_j = \sigma([\mathbf{W'}_r e(\mathbf{y}_{t-1})]_j + [\mathbf{U'}_r \mathbf{h'}_{\langle t-1 \rangle}]_j + [\mathbf{C}_r\mathbf{c}]_j)$$</p>
<p>其中$e(\mathbf{y}_0)$是一个全零向量。类似于encoder中的情况，$e(\mathbf{y})$也是目标词的embedding。</p>
<p>decoder需要学习如何生成一个目标短语。在$t$时刻，decoder需要计算生成的词是第$j$个的概率：</p>
<p>$$p(y_{t,j} = 1 | \mathbf{y}_{t-1}, ..., \mathbf{y}_1, X) = \frac{\exp{(\mathbf{g}_j \mathbf{s}_{\langle t \rangle}})}{\sum_{j'=1}^K \exp{(\mathbf{g}_{j'} \mathbf{s}_{\langle t \rangle})}}$$</p>
<p>其中$\mathbf{s}_{\langle t \rangle}$的第$i$个元素是</p>

$$\mathbf{s}_i^{\langle t \rangle} = \max{{{s'}_{2i-1}^{\langle t \rangle}, {s'}_{2i}^{\langle t \rangle}}}$$

<p>且</p>

$$\mathbf{s'}^{\langle t \rangle} = \mathbf{O}_h \mathbf{h'}^{\langle t \rangle} + \mathbf{O}_y \mathbf{y}_{t-1} + \mathbf{O}_c \mathbf{c}$$

<p>简单来说，$\mathbf{s}_i^{\langle t \rangle}$就是所谓的maxout单元。</p>
<p>（虽然我目前还不知道maxout是什么，以及这个$\mathbf{g}$是怎么来的，以及这一堆到底是怎么算的……）</p>
<p>为了计算效率，我们使用两个矩阵的乘积作为输出权重矩阵$\mathbf{G}$：</p>
<p>$$\mathbf{G} = \mathbf{G}_l \mathbf{G}_r$$</p>
<p>其中$\mathbf{G}_l \in \mathrm{R}^{K \times 500}$，$\mathbf{G}_r \in \mathrm{R}^{500 \times 1000}$。</p>

      
    </div>
    
      <div class="full-width auto-padding tags">
        
          <a href="/tags/Natural-Language-Processing/"><i class="fas fa-hashtag fa-fw"></i>Natural Language Processing</a>
        
          <a href="/tags/Reading-Report/"><i class="fas fa-hashtag fa-fw"></i>Reading Report</a>
        
          <a href="/tags/Machine-Learning/"><i class="fas fa-hashtag fa-fw"></i>Machine Learning</a>
        
          <a href="/tags/Paper/"><i class="fas fa-hashtag fa-fw"></i>Paper</a>
        
      </div>
    
  </section>
</article>

            </div>
          
        
          
            <div class='post-wrapper'>
              <article class="post reveal ">
  
<section class='meta'>
  
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/post/bleu-a-method-for-automatic-evaluation-of-machine-translation-acl2002/">
              
                  论文：BLEU: a Method for Automatic Evaluation of Machine Translation (ACL2002)
              
          </a>
      </h2>
    

    <div class='new-meta-box'>
      
        <div class='new-meta-item author'>
          <a href="https://zhanghuimeng.github.io">
            <i class="fas fa-user" aria-hidden="true"></i>
            张慕晖
          </a>
        </div>
      
      
        <div class="new-meta-item date">
          <a class='notlink'>
            <i class="fas fa-calendar-alt" aria-hidden="true"></i>
            2018-10-02
          </a>
        </div>
      
      
        
      
      
      
    </div>
    <hr>
  </div>
</section>

  <section class="article typo">
    <div class="article-entry" itemprop="articleBody">
      <p>论文地址：<a href="https://www.aclweb.org/anthology/P02-1040.pdf" target="_blank" rel="noopener">https://www.aclweb.org/anthology/P02-1040.pdf</a></p>
<p>这篇文章大概是MT领域非常著名的一篇文章了，因为它提出了一种根据参考翻译为机器翻译质量打分的方法（BLEU值），直到现在还在被广泛使用。</p>
<h2>基本思路</h2>
<p>BLEU评价翻译水平的基本假设是这样的：机器翻译越接近人类翻译，机器翻译的质量就越高。因此，在这一前提下，需要量化机器翻译与人类翻译的相似程度。经过观察可以发现，一个较好的机器翻译和人工翻译中相同的词和短语比较多；因此可以通过比较机器翻译和人工翻译中相同n-gram的数量来评价一个机器翻译。因此令$p_n$表示改进的n-gram精度（modified n-gram precision）：</p>
<p>$$p_n = \frac{\sum_{C \in {Candidates}} \sum_{n-gram \in C} Count_{clip}(n-gram)}{\sum_{C' \in {Candidates}} \sum_{n-gram' \in C'} Count(n-gram')}$$</p>
<p>其中$Count_{clip}(n-gram)$的意思是，对于某一个在机器翻译结果中出现的n-gram，它被统计的数量不超过该n-gram在某一个参考翻译中出现的总数量。这样可以防止出现机器翻译大量重复参考翻译中出现的某个词，结果却会得到较高分数的情况。</p>
<p><img src="figure1.png" alt="人类翻译和机器翻译的得分"></p>
<p>此时可以看出，这一评分方法可以区分人类和机器翻译了，且n越大，得分的差异越大。</p>
<p><img src="figure2.png" alt="不同水平人类翻译和机器翻译的得分"></p>
<p>此时可以看出，这一评分方法也可以在较细粒度上区分人类和机器翻译。</p>
<h3>改进：组合多种n-gram的得分</h3>
<p>可以看出，n-gram分数大致随n的增加指数衰减，所以对$p_n$的对数做加权平均是比较好的。实验得出，取$n \leq 4$时得到的评测结果最为接近。</p>
<h3>改进：短句惩罚</h3>
<p>很显然上面的做法已经惩罚了过长的句子和被使用太多次的词，但是并没有考虑到句子太短的情况。只考虑精度可能会使得短句得到非常高的分数。但是考虑召回率又可能会使得事情变得过于复杂。所以直接考虑机器翻译和参考翻译的长度。对于每个机器翻译得到的句子，在（可能有多个的）参考翻译中找到与它长度最为接近的句子，称其长度为最佳匹配长度（best match length）。然后计算出整体语料的机器翻译最佳匹配长度之和，称为$r$；令$c$是机器翻译结果的总长度，定义短句惩罚（brevity penalty）为</p>
<p>$$<br>
\text{BP} =<br>
\begin{cases}<br>
1 &amp; \text{if} \: c &gt; r\\<br>
e^{(1 - r/c)} &amp; \text{if} \: c \leq r<br>
\end{cases}<br>
$$</p>
<p>将这个因子乘到BLEU分数上：</p>
<p>$$\text{BLEU} = \text{BP} \cdot \exp{(\sum_{n=1}^{N} w_n \log{p_n})}$$</p>
<p>即</p>
<p>$$\log{\text{BLEU}} = \min{(1 - \frac{r}{c}, 0)} + \sum_{n=1}^{N} w_n \log{p_n}$$</p>
<h2>测试</h2>
<p>在测试中，取$N = 4$，$w_n = 1/N$（也就是直接平均）。</p>
<p>作者首先回答了这样几个问题：</p>
<ul>
<li>（不同机器/人类翻译）BLEU值的差异是可信的吗？</li>
<li>（不同机器/人类翻译）BLEU值的差异是稳定的吗？</li>
<li>BLEU值的方差是多少？</li>
</ul>
<p><img src="table1-2.png" alt="500个句子"></p>
<p>作者找了500个句子，每个各有4个参考翻译，首先让机器/人类分别翻译500个句子，分别计算其BLEU值；然后把这500个句子分成20组，分组计算BLEU值，并计算出平均值和标准差，并对不同系统翻译的结果进行结对T检验。即使只随机留下每个句子的一个参考翻译，BLEU值的排序也没有改变。这说明BLEU值的差异是可信和稳定的，且方差不是很大。</p>
<p>之后作者比较了BLEU评测和人类评测（之前对人类评测也做了一个结对T检验，虽然我并不知道有什么用）的结果。</p>
<p><img src="figure5.png" alt="BLEU值和单语言评测者打分的比较"></p>
<p><img src="figure6.png" alt="BLEU值和多语言评测者打分的比较"></p>
<p><img src="figure7.png" alt="BLEU值和两种评测者的比较"></p>
<p>可以看出，BLEU值和单语言评测者的打分比较相近，和多语言评测者的打分差别比较大。这可能是因为BLEU值只考虑了翻译结果的流利度等因素，不像多语言评测者那样，会考虑更多的语义方面的问题。</p>
<h2>总结</h2>
<p>这个评价方法主要考虑的是翻译结果和参考翻译之间的相似程度，用比较简单的方法得到了与人类评价相似的结果。不过我的问题是：</p>
<ol>
<li>单纯用n-gram计数的比较是否过于简单，没有考虑参考翻译和机器翻译之间语义比较的问题？</li>
<li>BLEU和单语言评测者比较接近大概是有原因的：这一评测方法完全依赖于参考翻译，完全没有管源句的结构之类的问题。不过我觉得只要能保证参考翻译是高质量的，这并不能说是一件坏事，这应该取决于MT的主要应用场景。</li>
</ol>

      
    </div>
    
      <div class="full-width auto-padding tags">
        
          <a href="/tags/Natural-Language-Processing/"><i class="fas fa-hashtag fa-fw"></i>Natural Language Processing</a>
        
          <a href="/tags/Reading-Report/"><i class="fas fa-hashtag fa-fw"></i>Reading Report</a>
        
          <a href="/tags/Machine-Translation/"><i class="fas fa-hashtag fa-fw"></i>Machine Translation</a>
        
          <a href="/tags/Paper/"><i class="fas fa-hashtag fa-fw"></i>Paper</a>
        
      </div>
    
  </section>
</article>

            </div>
          
        
      

  </section>






  

  <!-- 根据主题中的设置决定是否在archive中针对摘要部分的MathJax公式加载mathjax.js文件 -->
  

  
    <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": {
      preferredFont: "TeX",
      availableFonts: ["STIX","TeX"],
      linebreaks: { automatic:true },
      EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
      inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
      processEscapes: true,
      ignoreClass: "tex2jax_ignore|dno",
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: { autoNumber: "AMS" },
      noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
      Macros: { href: "{}" }
    },
    messageStyle: "none"
  });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += (all[i].SourceElement().parentNode.className ? ' ' : '') + 'has-jax';
    }
    console.log("mathjax did loaded!");
  });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

  



        </div>
        <aside class='l_side'>
            
  
  
    
      
      
        <section class='author'>
  <div class='content pure'>
    
    
    
      <div class="social-wrapper">
        
          
            <a href="mailto:zhanghuimeng1997@gmail.com" class="social flat-btn" target="_blank" rel="external"><i class="social fas fa-envelope" aria-hidden="true"></i></a>
          
        
          
            <a href="https://github.com/zhanghuimeng" class="social flat-btn" target="_blank" rel="external"><i class="social fab fa-github" aria-hidden="true"></i></a>
          
        
          
            <a href="https://music.163.com/#/user/home?id=261028414" class="social flat-btn" target="_blank" rel="external"><i class="social fas fa-music" aria-hidden="true"></i></a>
          
        
      </div>
    
  </div>
</section>

      
    
  
    
      
      
        

      
    
  
    
      
      
        
  <section class='category'>
    
<header class='pure'>
  <div><i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;所有分类</div>
  
</header>

    <div class='content pure'>
      <ul class="entry">
        
          <li><a class="flat-box" title="/categories/Blogging/" href="/categories/Blogging/"><div class='name'>Blogging</div><div class='badge'>(2)</div></a></li>
        
          <li><a class="flat-box" title="/categories/Codeforces/" href="/categories/Codeforces/"><div class='name'>Codeforces</div><div class='badge'>(4)</div></a></li>
        
          <li><a class="flat-box" title="/categories/Leetcode/" href="/categories/Leetcode/"><div class='name'>Leetcode</div><div class='badge'>(31)</div></a></li>
        
          <li><a class="flat-box" title="/categories/MLDS/" href="/categories/MLDS/"><div class='name'>MLDS</div><div class='badge'>(0)</div></a></li>
        
          <li><a class="flat-box" title="/categories/NLP/" href="/categories/NLP/"><div class='name'>NLP</div><div class='badge'>(1)</div></a></li>
        
          <li><a class="flat-box" title="/categories/USACO/" href="/categories/USACO/"><div class='name'>USACO</div><div class='badge'>(5)</div></a></li>
        
          <li><a class="flat-box" title="/categories/博客/" href="/categories/博客/"><div class='name'>博客</div><div class='badge'>(0)</div></a></li>
        
          <li><a class="flat-box" title="/categories/旧博客/" href="/categories/旧博客/"><div class='name'>旧博客</div><div class='badge'>(2)</div></a></li>
        
          <li><a class="flat-box" title="/categories/深度学习/" href="/categories/深度学习/"><div class='name'>深度学习</div><div class='badge'>(0)</div></a></li>
        
      </ul>
    </div>
  </section>


      
    
  
    
      
      
        
  <section class='tagcloud'>
    
<header class='pure'>
  <div><i class="fas fa-hashtag fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;热门标签</div>
  
</header>

    <div class='content pure'>
      <a href="/tags/A-Munday/" style="font-size: 14px; color: #999">A.Munday</a> <a href="/tags/Blogging/" style="font-size: 14px; color: #999">Blogging</a> <a href="/tags/C-Marlowe/" style="font-size: 14px; color: #999">C.Marlowe</a> <a href="/tags/CSP/" style="font-size: 15.07px; color: #929292">CSP</a> <a href="/tags/Codeforces/" style="font-size: 19.36px; color: #757575">Codeforces</a> <a href="/tags/Codeforces-Contest/" style="font-size: 19px; color: #777">Codeforces Contest</a> <a href="/tags/Counseling/" style="font-size: 14px; color: #999">Counseling</a> <a href="/tags/Cryptography/" style="font-size: 14px; color: #999">Cryptography</a> <a href="/tags/D-Drayton/" style="font-size: 14px; color: #999">D.Drayton</a> <a href="/tags/Deep-Learning/" style="font-size: 14px; color: #999">Deep Learning</a> <a href="/tags/Depth-first-Search/" style="font-size: 14px; color: #999">Depth-first Search</a> <a href="/tags/DigitCircuit/" style="font-size: 14px; color: #999">DigitCircuit</a> <a href="/tags/E-Vere/" style="font-size: 14px; color: #999">E. Vere</a> <a href="/tags/E-Spencer/" style="font-size: 14px; color: #999">E.Spencer</a> <a href="/tags/Essay/" style="font-size: 14.36px; color: #979797">Essay</a> <a href="/tags/Flask/" style="font-size: 14px; color: #999">Flask</a> <a href="/tags/Github/" style="font-size: 14.71px; color: #949494">Github</a> <a href="/tags/GoldenTreasury/" style="font-size: 23.29px; color: #5a5a5a">GoldenTreasury</a> <a href="/tags/Google-Analytics/" style="font-size: 14px; color: #999">Google Analytics</a> <a href="/tags/H-Constable/" style="font-size: 14px; color: #999">H.Constable</a> <a href="/tags/Hexo/" style="font-size: 14px; color: #999">Hexo</a> <a href="/tags/J-Donne/" style="font-size: 14px; color: #999">J.Donne</a> <a href="/tags/J-Lyly/" style="font-size: 14px; color: #999">J.Lyly</a> <a href="/tags/J-Sylvester/" style="font-size: 14px; color: #999">J.Sylvester</a> <a href="/tags/J-Webster/" style="font-size: 14px; color: #999">J.Webster</a> <a href="/tags/Leetcode/" style="font-size: 24px; color: #555">Leetcode</a> <a href="/tags/Leetcode-Contest/" style="font-size: 23.64px; color: #575757">Leetcode Contest</a> <a href="/tags/Lyric/" style="font-size: 17.21px; color: #838383">Lyric</a> <a href="/tags/Machine-Learning/" style="font-size: 15.07px; color: #929292">Machine Learning</a> <a href="/tags/Machine-Translation/" style="font-size: 16.5px; color: #888">Machine Translation</a> <a href="/tags/NLP/" style="font-size: 14px; color: #999">NLP</a> <a href="/tags/Natural-Language-Processing/" style="font-size: 17.21px; color: #838383">Natural Language Processing</a> <a href="/tags/OS/" style="font-size: 21.14px; color: #686868">OS</a> <a href="/tags/OSTEP/" style="font-size: 17.93px; color: #7e7e7e">OSTEP</a> <a href="/tags/Old-Blog/" style="font-size: 14px; color: #999">Old Blog</a> <a href="/tags/OldBlog/" style="font-size: 14.71px; color: #949494">OldBlog</a> <a href="/tags/P-Sidney/" style="font-size: 14px; color: #999">P.Sidney</a> <a href="/tags/Paper/" style="font-size: 16.5px; color: #888">Paper</a> <a href="/tags/Paul-Simon/" style="font-size: 14px; color: #999">Paul Simon</a> <a href="/tags/PhysicsExperiment/" style="font-size: 14px; color: #999">PhysicsExperiment</a> <a href="/tags/Psychology/" style="font-size: 14px; color: #999">Psychology</a> <a href="/tags/PyCharm/" style="font-size: 14px; color: #999">PyCharm</a> <a href="/tags/Quality-Estimation/" style="font-size: 15.43px; color: #8f8f8f">Quality Estimation</a> <a href="/tags/R-Barnfield/" style="font-size: 14px; color: #999">R.Barnfield</a> <a href="/tags/Raspberry-Pi/" style="font-size: 14px; color: #999">Raspberry Pi</a> <a href="/tags/Reading-Report/" style="font-size: 17.57px; color: #818181">Reading Report</a> <a href="/tags/S-Daniel/" style="font-size: 14px; color: #999">S.Daniel</a> <a href="/tags/SGU/" style="font-size: 14.36px; color: #979797">SGU</a> <a href="/tags/Sonnet/" style="font-size: 19.71px; color: #727272">Sonnet</a> <a href="/tags/Spokes/" style="font-size: 14.71px; color: #949494">Spokes</a> <a href="/tags/SystemAnalysis-Control/" style="font-size: 14px; color: #999">SystemAnalysis&Control</a> <a href="/tags/T-Dekker/" style="font-size: 14px; color: #999">T.Dekker</a> <a href="/tags/T-Heywood/" style="font-size: 14px; color: #999">T.Heywood</a> <a href="/tags/T-Lodge/" style="font-size: 14px; color: #999">T.Lodge</a> <a href="/tags/T-Nashe/" style="font-size: 14px; color: #999">T.Nashe</a> <a href="/tags/T-Wyatt/" style="font-size: 14px; color: #999">T.Wyatt</a> <a href="/tags/THUMT/" style="font-size: 14.36px; color: #979797">THUMT</a> <a href="/tags/TensorFlow/" style="font-size: 15.07px; color: #929292">TensorFlow</a> <a href="/tags/Translation/" style="font-size: 18.64px; color: #797979">Translation</a> <a href="/tags/Tree/" style="font-size: 14px; color: #999">Tree</a> <a href="/tags/USACO/" style="font-size: 22.21px; color: #616161">USACO</a> <a href="/tags/W-Alexander/" style="font-size: 14px; color: #999">W.Alexander</a> <a href="/tags/W-Drummond/" style="font-size: 15.07px; color: #929292">W.Drummond</a> <a href="/tags/W-Shakespeare/" style="font-size: 21.5px; color: #666">W.Shakespeare</a> <a href="/tags/WebStorm/" style="font-size: 14px; color: #999">WebStorm</a> <a href="/tags/object-Object/" style="font-size: 14px; color: #999">[object Object]</a> <a href="/tags/alg-Ad-Hoc/" style="font-size: 14.36px; color: #979797">alg:Ad-Hoc</a> <a href="/tags/alg-Aho–Corasick-Algorithm/" style="font-size: 14px; color: #999">alg:Aho–Corasick Algorithm</a> <a href="/tags/alg-Array/" style="font-size: 20.79px; color: #6b6b6b">alg:Array</a> <a href="/tags/alg-Automata/" style="font-size: 14px; color: #999">alg:Automata</a> <a href="/tags/alg-Backtracking/" style="font-size: 15.79px; color: #8d8d8d">alg:Backtracking</a> <a href="/tags/alg-Binary-Indexed-Tree/" style="font-size: 14px; color: #999">alg:Binary Indexed Tree</a> <a href="/tags/alg-Binary-Search/" style="font-size: 16.5px; color: #888">alg:Binary Search</a> <a href="/tags/alg-Binary-Search-Tree/" style="font-size: 16.86px; color: #868686">alg:Binary Search Tree</a> <a href="/tags/alg-Binary-Tree/" style="font-size: 14px; color: #999">alg:Binary Tree</a> <a href="/tags/alg-Binray-Search/" style="font-size: 14px; color: #999">alg:Binray Search</a> <a href="/tags/alg-Bit-Manipulation/" style="font-size: 15.43px; color: #8f8f8f">alg:Bit Manipulation</a> <a href="/tags/alg-Bitmasks/" style="font-size: 14px; color: #999">alg:Bitmasks</a> <a href="/tags/alg-Breadth-First-Search/" style="font-size: 14px; color: #999">alg:Breadth-First Search</a> <a href="/tags/alg-Breadth-first-Search/" style="font-size: 18.29px; color: #7c7c7c">alg:Breadth-first Search</a> <a href="/tags/alg-Breadth-firth-Search/" style="font-size: 14.36px; color: #979797">alg:Breadth-firth Search</a> <a href="/tags/alg-Brute-Force/" style="font-size: 17.21px; color: #838383">alg:Brute Force</a> <a href="/tags/alg-Centroid-Decomposition/" style="font-size: 14px; color: #999">alg:Centroid Decomposition</a> <a href="/tags/alg-Depth-first-Search/" style="font-size: 20.07px; color: #707070">alg:Depth-first Search</a> <a href="/tags/alg-Divide-and-Conquer/" style="font-size: 14px; color: #999">alg:Divide and Conquer</a> <a href="/tags/alg-Dynamic-Porgramming/" style="font-size: 14px; color: #999">alg:Dynamic Porgramming</a> <a href="/tags/alg-Dynamic-Programming/" style="font-size: 22.57px; color: #5f5f5f">alg:Dynamic Programming</a> <a href="/tags/alg-Games/" style="font-size: 14px; color: #999">alg:Games</a> <a href="/tags/alg-Geometry/" style="font-size: 14px; color: #999">alg:Geometry</a> <a href="/tags/alg-Graph/" style="font-size: 15.43px; color: #8f8f8f">alg:Graph</a> <a href="/tags/alg-Greedy/" style="font-size: 21.86px; color: #646464">alg:Greedy</a> <a href="/tags/alg-Hash-Table/" style="font-size: 19.71px; color: #727272">alg:Hash Table</a> <a href="/tags/alg-Heap/" style="font-size: 15.43px; color: #8f8f8f">alg:Heap</a> <a href="/tags/alg-In-Order-Traversal/" style="font-size: 14.36px; color: #979797">alg:In-Order Traversal</a> <a href="/tags/alg-Index-Search-Array/" style="font-size: 14px; color: #999">alg:Index Search Array</a> <a href="/tags/alg-Linked-List/" style="font-size: 15.79px; color: #8d8d8d">alg:Linked List</a> <a href="/tags/alg-Map/" style="font-size: 14px; color: #999">alg:Map</a> <a href="/tags/alg-Math/" style="font-size: 22.93px; color: #5c5c5c">alg:Math</a> <a href="/tags/alg-Matrix/" style="font-size: 14px; color: #999">alg:Matrix</a> <a href="/tags/alg-Meet-in-the-Middle/" style="font-size: 14.36px; color: #979797">alg:Meet in the Middle</a> <a href="/tags/alg-Minimax/" style="font-size: 14.36px; color: #979797">alg:Minimax</a> <a href="/tags/alg-Minmax/" style="font-size: 14px; color: #999">alg:Minmax</a> <a href="/tags/alg-Monotonic-Stack/" style="font-size: 16.14px; color: #8a8a8a">alg:Monotonic Stack</a> <a href="/tags/alg-Network-Flow/" style="font-size: 14px; color: #999">alg:Network Flow</a> <a href="/tags/alg-Priority-Queue/" style="font-size: 14px; color: #999">alg:Priority Queue</a> <a href="/tags/alg-Queue/" style="font-size: 14.71px; color: #949494">alg:Queue</a> <a href="/tags/alg-Rabin-Karp/" style="font-size: 14px; color: #999">alg:Rabin-Karp</a> <a href="/tags/alg-Random/" style="font-size: 14.71px; color: #949494">alg:Random</a> <a href="/tags/alg-Rank-Tree/" style="font-size: 14px; color: #999">alg:Rank Tree</a> <a href="/tags/alg-Recursion/" style="font-size: 15.43px; color: #8f8f8f">alg:Recursion</a> <a href="/tags/alg-Recursive/" style="font-size: 14.36px; color: #979797">alg:Recursive</a> <a href="/tags/alg-Rejection-Sampling/" style="font-size: 14px; color: #999">alg:Rejection Sampling</a> <a href="/tags/alg-Reservoir-Sampling/" style="font-size: 14px; color: #999">alg:Reservoir Sampling</a> <a href="/tags/alg-Segmentation-Tree/" style="font-size: 14px; color: #999">alg:Segmentation Tree</a> <a href="/tags/alg-Set/" style="font-size: 14px; color: #999">alg:Set</a> <a href="/tags/alg-Sliding-Window/" style="font-size: 14px; color: #999">alg:Sliding Window</a> <a href="/tags/alg-Sort/" style="font-size: 15.07px; color: #929292">alg:Sort</a> <a href="/tags/alg-Stack/" style="font-size: 19px; color: #777">alg:Stack</a> <a href="/tags/alg-String/" style="font-size: 19px; color: #777">alg:String</a> <a href="/tags/alg-Suffix-Array/" style="font-size: 14px; color: #999">alg:Suffix Array</a> <a href="/tags/alg-Suffix-Tree/" style="font-size: 14px; color: #999">alg:Suffix Tree</a> <a href="/tags/alg-Ternary-Search/" style="font-size: 14px; color: #999">alg:Ternary Search</a> <a href="/tags/alg-Topological-Sort/" style="font-size: 14px; color: #999">alg:Topological Sort</a> <a href="/tags/alg-Treap/" style="font-size: 14px; color: #999">alg:Treap</a> <a href="/tags/alg-Tree/" style="font-size: 20.43px; color: #6d6d6d">alg:Tree</a> <a href="/tags/alg-Trie/" style="font-size: 14.36px; color: #979797">alg:Trie</a> <a href="/tags/alg-Two-Pointers/" style="font-size: 17.93px; color: #7e7e7e">alg:Two Pointers</a> <a href="/tags/alg-Union-find-Forest/" style="font-size: 15.43px; color: #8f8f8f">alg:Union-find Forest</a> <a href="/tags/artist-Ceremony/" style="font-size: 14px; color: #999">artist:Ceremony</a> <a href="/tags/artist-Cruel-Hand/" style="font-size: 14.36px; color: #979797">artist:Cruel Hand</a> <a href="/tags/artist-Have-Heart/" style="font-size: 14px; color: #999">artist:Have Heart</a> <a href="/tags/artist-Johnny-Cash/" style="font-size: 14px; color: #999">artist:Johnny Cash</a> <a href="/tags/artist-Touche-Amore/" style="font-size: 14px; color: #999">artist:Touche Amore</a> <a href="/tags/artist-Wir-Sind-Helden/" style="font-size: 14.71px; color: #949494">artist:Wir Sind Helden</a> <a href="/tags/translation/" style="font-size: 14.36px; color: #979797">translation</a> <a href="/tags/ucore/" style="font-size: 14px; color: #999">ucore</a> <a href="/tags/付勇林/" style="font-size: 15.79px; color: #8d8d8d">付勇林</a> <a href="/tags/卞之琳/" style="font-size: 14px; color: #999">卞之琳</a> <a href="/tags/屠岸/" style="font-size: 16.14px; color: #8a8a8a">屠岸</a> <a href="/tags/戴镏龄/" style="font-size: 15.79px; color: #8d8d8d">戴镏龄</a> <a href="/tags/曹明伦/" style="font-size: 15.43px; color: #8f8f8f">曹明伦</a> <a href="/tags/朱生豪/" style="font-size: 17.57px; color: #818181">朱生豪</a> <a href="/tags/李霁野/" style="font-size: 15.07px; color: #929292">李霁野</a> <a href="/tags/杨熙龄/" style="font-size: 14px; color: #999">杨熙龄</a> <a href="/tags/林天斗/" style="font-size: 14px; color: #999">林天斗</a> <a href="/tags/梁宗岱/" style="font-size: 16.86px; color: #868686">梁宗岱</a> <a href="/tags/梁葆成/" style="font-size: 14px; color: #999">梁葆成</a> <a href="/tags/袁广达/" style="font-size: 14px; color: #999">袁广达</a> <a href="/tags/郭沫若/" style="font-size: 14px; color: #999">郭沫若</a> <a href="/tags/黄新渠/" style="font-size: 14px; color: #999">黄新渠</a>
    </div>
  </section>


      
    
  
    
      
      
        <section class='list'>
  
<header class='pure'>
  <div><i class="fas fa-link fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;特别链接</div>
  
</header>

  <div class='content pure'>
    <ul class="entry">
      
        <li><a class="flat-box" title="https://wenj.github.io/" href="https://wenj.github.io/">
          <div class='name'>
            
              <i class="fas fa-comment-dots fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;wenj
          </div>
          
        </a></li>
      
        <li><a class="flat-box" title="http://bellasong.site/" href="http://bellasong.site/">
          <div class='name'>
            
              <i class="fas fa-comment-dots fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;ssh
          </div>
          
        </a></li>
      
    </ul>
  </div>
</section>

      
    
  


        </aside>
        <script>setLoadingBarProgress(60);</script>
    </div>
    <a class="s-top fas fa-arrow-up fa-fw" href='javascript:void(0)'></a>
    </div>
    <footer id="footer" class="clearfix">
  
  
    <div class="social-wrapper">
      
        
          <a href="mailto:zhanghuimeng1997@gmail.com" class="social fas fa-envelope flat-btn" target="_blank" rel="external"></a>
        
      
        
          <a href="https://github.com/zhanghuimeng" class="social fab fa-github flat-btn" target="_blank" rel="external"></a>
        
      
        
          <a href="https://music.163.com/#/user/home?id=261028414" class="social fas fa-music flat-btn" target="_blank" rel="external"></a>
        
      
    </div>
  
  <br>
  <div><p>博客内容遵循 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">署名-非商业性使用-相同方式共享 4.0 国际 (CC BY-NC-SA 4.0) 协议</a></p>
</div>
  <div>本站使用 <a href="https://xaoxuu.com/wiki/material-x/" target="_blank" class="codename">Material X</a> 作为主题，总访问量为 <span id="busuanzi_value_site_pv"><i class="fas fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span> 次。
  </div>
</footer>

    <script>setLoadingBarProgress(80);</script>
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js"></script>

  <script>
    var GOOGLE_CUSTOM_SEARCH_API_KEY = "";
    var GOOGLE_CUSTOM_SEARCH_ENGINE_ID = "";
    var ALGOLIA_API_KEY = "";
    var ALGOLIA_APP_ID = "";
    var ALGOLIA_INDEX_NAME = "";
    var AZURE_SERVICE_NAME = "";
    var AZURE_INDEX_NAME = "";
    var AZURE_QUERY_KEY = "";
    var BAIDU_API_ID = "";
    var SEARCH_SERVICE = "hexo" || "hexo";
    var ROOT = "/"||"/";
    if(!ROOT.endsWith('/'))ROOT += '/';
  </script>



  <script async src="https://cdn.jsdelivr.net/npm/scrollreveal@4.0.5/dist/scrollreveal.min.js"></script>
  <script type="text/javascript">
    $(function() {
      const $reveal = $('.reveal');
      if ($reveal.length === 0) return;
      const sr = ScrollReveal({ distance: 0 });
      sr.reveal('.reveal');
    });
  </script>


  <script src="https://cdn.jsdelivr.net/npm/node-waves@0.7.6/dist/waves.min.js"></script>
  <script type="text/javascript">
    $(function() {
      Waves.attach('.flat-btn', ['waves-button']);
      Waves.attach('.float-btn', ['waves-button', 'waves-float']);
      Waves.attach('.float-btn-light', ['waves-button', 'waves-float', 'waves-light']);
      Waves.attach('.flat-box', ['waves-block']);
      Waves.attach('.float-box', ['waves-block', 'waves-float']);
      Waves.attach('.waves-image');
      Waves.init();
    });
  </script>


  <script async src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-busuanzi@2.3/js/busuanzi.pure.mini.js"></script>







  <script type="text/javascript">
    (function(d, s) {
      var j, e = d.getElementsByTagName(s)[0];
      if (typeof LivereTower === 'function') { return; }
      j = d.createElement(s);
      j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
      j.async = true;
      e.parentNode.insertBefore(j, e);
    })(document, 'script');
  </script>





  <script src="/js/app.js"></script>
<script src="/js/search.js"></script>





<!-- 复制 -->
<script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  let COPY_SUCCESS = "复制成功";
  let COPY_FAILURE = "复制失败";
  /*页面载入完成后，创建复制按钮*/
  !function (e, t, a) {
    /* code */
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '  <i class="fa fa-copy"></i><span>Copy</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });

      clipboard.on('success', function(e) {
        //您可以加入成功提示
        console.info('Action:', e.action);
        console.info('Text:', e.text);
        console.info('Trigger:', e.trigger);
        success_prompt(COPY_SUCCESS);
        e.clearSelection();
      });
      clipboard.on('error', function(e) {
        //您可以加入失败提示
        console.error('Action:', e.action);
        console.error('Trigger:', e.trigger);
        fail_prompt(COPY_FAILURE);
      });
    }
    initCopyCode();

  }(window, document);

  /**
   * 弹出式提示框，默认1.5秒自动消失
   * @param message 提示信息
   * @param style 提示样式，有alert-success、alert-danger、alert-warning、alert-info
   * @param time 消失时间
   */
  var prompt = function (message, style, time)
  {
      style = (style === undefined) ? 'alert-success' : style;
      time = (time === undefined) ? 1500 : time*1000;
      $('<div>')
          .appendTo('body')
          .addClass('alert ' + style)
          .html(message)
          .show()
          .delay(time)
          .fadeOut();
  };

  // 成功提示
  var success_prompt = function(message, time)
  {
      prompt(message, 'alert-success', time);
  };

  // 失败提示
  var fail_prompt = function(message, time)
  {
      prompt(message, 'alert-danger', time);
  };

  // 提醒
  var warning_prompt = function(message, time)
  {
      prompt(message, 'alert-warning', time);
  };

  // 信息提示
  var info_prompt = function(message, time)
  {
      prompt(message, 'alert-info', time);
  };

</script>


<!-- fancybox -->
<script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>
<script>
  let LAZY_LOAD_IMAGE = "";
  $(".article-entry").find("img").each(function () {
      var element = document.createElement("a");
      $(element).attr("data-fancybox", "gallery");
      $(element).attr("href", $(this).attr("src"));
      /* 图片采用懒加载处理时,
       * 一般图片标签内会有个属性名来存放图片的真实地址，比如 data-original,
       * 那么此处将原本的属性名src替换为对应属性名data-original,
       * 修改如下
       */
       if (LAZY_LOAD_IMAGE) {
         $(element).attr("href", $(this).attr("data-original"));
       }
      $(this).wrap(element);
  });
</script>





    <script>setLoadingBarProgress(100);</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
